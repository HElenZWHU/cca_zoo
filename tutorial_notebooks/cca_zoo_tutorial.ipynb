{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "cca-zoo-tutorial.ipynb",
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jameschapman19/cca_zoo/blob/master/cca_zoo_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFVm_oT3cmZH",
    "outputId": "745b34d4-1258-4ec4-df96-1f52c235ecaf"
   },
   "source": [
    "!pip install cca-zoo --upgrade\n",
    "!pip install scipy --upgrade"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting cca-zoo\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/7d/b4/dfe8d6b8e1c8f2a54f543282ce1e02c0dda80922c20a13dacd2b05367850/cca_zoo-1.2.6-py3-none-any.whl (45kB)\n",
      "\r\u001B[K     |███████▏                        | 10kB 10.9MB/s eta 0:00:01\r\u001B[K     |██████████████▍                 | 20kB 15.6MB/s eta 0:00:01\r\u001B[K     |█████████████████████▌          | 30kB 11.4MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▊   | 40kB 7.9MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 51kB 3.0MB/s \n",
      "\u001B[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from cca-zoo) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from cca-zoo) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from cca-zoo) (3.2.2)\n",
      "Collecting tensorly\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/42/3a/bf39a426e0d13a60d8cdee42419edeaa24aed2ec82f90a820b040e7db190/tensorly-0.6.0-py3-none-any.whl (160kB)\n",
      "\u001B[K     |████████████████████████████████| 163kB 8.0MB/s \n",
      "\u001B[?25hRequirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from cca-zoo) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.7/dist-packages (from cca-zoo) (0.11.1)\n",
      "Collecting scipy>=1.5\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/91/ee427c42957f8c4cbe477bf4f8b7f608e003a17941e509d1777e58648cb3/scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
      "\u001B[K     |████████████████████████████████| 27.4MB 134kB/s \n",
      "\u001B[?25hRequirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.7/dist-packages (from cca-zoo) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->cca-zoo) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->cca-zoo) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cca-zoo) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cca-zoo) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cca-zoo) (0.10.0)\n",
      "Collecting nose\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
      "\u001B[K     |████████████████████████████████| 163kB 34.3MB/s \n",
      "\u001B[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->cca-zoo) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->cca-zoo) (1.15.0)\n",
      "\u001B[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001B[0m\n",
      "Installing collected packages: scipy, nose, tensorly, cca-zoo\n",
      "  Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "Successfully installed cca-zoo-1.2.6 nose-1.3.7 scipy-1.6.2 tensorly-0.6.0\n",
      "Requirement already up-to-date: scipy in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jFsRNCJ0crH8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9eafd6c9-ad2e-41e6-c8a7-69f3283be80a"
   },
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from cca_zoo.models import CCA,CCA_ALS,PLS,PMD\n",
    "from cca_zoo.data import Noisy_MNIST_Dataset, simulated\n",
    "import itertools\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "from torch import optim\n",
    "\n",
    "# Load MNIST Data\n",
    "os.chdir('..')\n",
    "N = 500\n",
    "dataset = Noisy_MNIST_Dataset(mnist_type='FashionMNIST', train=True)\n",
    "ids = np.arange(min(2 * N, len(dataset)))\n",
    "np.random.shuffle(ids)\n",
    "train_ids, val_ids = np.array_split(ids, 2)\n",
    "val_dataset = Subset(dataset, val_ids)\n",
    "train_dataset = Subset(dataset, train_ids)\n",
    "test_dataset = Noisy_MNIST_Dataset(mnist_type='FashionMNIST', train=False)\n",
    "test_ids = np.arange(min(N, len(test_dataset)))\n",
    "np.random.shuffle(test_ids)\n",
    "test_dataset = Subset(test_dataset, test_ids)\n",
    "train_view_1, train_view_2, train_rotations, train_OH_labels, train_labels = train_dataset.dataset.to_numpy(\n",
    "    train_dataset.indices)\n",
    "val_view_1, val_view_2, val_rotations, val_OH_labels, val_labels = val_dataset.dataset.to_numpy(val_dataset.indices)\n",
    "test_view_1, test_view_2, test_rotations, test_OH_labels, test_labels = test_dataset.dataset.to_numpy(\n",
    "    test_dataset.indices)\n",
    "\n",
    "# Settings\n",
    "\n",
    "# The number of latent dimensions across models\n",
    "latent_dims = 2\n",
    "# The number of folds used for cross-validation/hyperparameter tuning\n",
    "cv_folds = 5\n",
    "# For running hyperparameter tuning in parallel (0 if not)\n",
    "jobs = 2\n",
    "# Number of iterations for iterative algorithms\n",
    "max_iter = 2\n",
    "# number of epochs for deep models\n",
    "epochs = 50"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOryUM6ShmQf"
   },
   "source": [
    "# Canonical Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZSGgSD7vgh1b"
   },
   "source": [
    "\"\"\"\n",
    "### Linear CCA by eigendecomposition\n",
    "\"\"\"\n",
    "linear_cca = CCA(latent_dims=latent_dims)\n",
    "\n",
    "linear_cca.fit(train_view_1, train_view_2)\n",
    "\n",
    "linear_cca_results = np.stack(\n",
    "    (linear_cca.train_correlations[0, 1], linear_cca.predict_corr(test_view_1, test_view_2)[0, 1]))\n",
    "\n",
    "\"\"\"\n",
    "### Linear CCA by alternating least squares (can pass more than 2 views)\n",
    "\"\"\"\n",
    "\n",
    "linear_cca_als = CCA_ALS(latent_dims=latent_dims)\n",
    "\n",
    "linear_cca_als.fit(train_view_1, train_view_2)\n",
    "\n",
    "linear_cca_als_results = np.stack(\n",
    "    (linear_cca_als.train_correlations[0, 1], linear_cca_als.predict_corr(test_view_1, test_view_2)[0, 1]))"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZtZUfZ3huKE"
   },
   "source": [
    "# Partial Least Squares\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ei3rASx3ghtJ"
   },
   "source": [
    "\"\"\"\n",
    "### PLS with scikit-learn (only permits 2 views)\n",
    "\"\"\"\n",
    "pls = PLS(latent_dims=latent_dims)\n",
    "\n",
    "pls.fit(train_view_1, train_view_2)\n",
    "\n",
    "pls_results = np.stack(\n",
    "    (pls.train_correlations[0, 1], pls.predict_corr(test_view_1, test_view_2)[0, 1]))"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88vAaUxIi1H7"
   },
   "source": [
    "# Extension to multiple views\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Fx0dj8GghbD"
   },
   "source": [
    "\"\"\"\n",
    "### (Regularized) Generalized CCA(can pass more than 2 views)\n",
    "\"\"\"\n",
    "# small ammount of regularisation added since data is not full rank\n",
    "c=[0.5,0.5,0.5]\n",
    "\n",
    "gcca = GCCA(latent_dims=latent_dims,c=c)\n",
    "\n",
    "gcca.fit(train_view_1, train_view_2,train_view_1)\n",
    "\n",
    "gcca_results = np.stack((gcca.train_correlations[0, 1], gcca.predict_corr(test_view_1, test_view_2)[0, 1]))\n",
    "\n",
    "\"\"\"\n",
    "### (Regularized) Multiset CCA(can pass more than 2 views)\n",
    "\"\"\"\n",
    "\n",
    "mcca = MCCA(latent_dims=latent_dims, c=c)\n",
    "\n",
    "mcca.fit(train_view_1, train_view_2,train_view_1)\n",
    "\n",
    "mcca_results = np.stack((mcca.train_correlations[0, 1], mcca.predict_corr(test_view_1, test_view_2)[0, 1]))\n",
    "\n",
    "\"\"\"\n",
    "### Multiset CCA by alternating least squares\n",
    "\"\"\"\n",
    "mcca_als = CCA_ALS(latent_dims=latent_dims, max_iter=max_iter)\n",
    "\n",
    "mcca_als.fit(train_view_1, train_view_2,train_view_1)\n",
    "\n",
    "mcca_als_results = np.stack(\n",
    "    (mcca_als.train_correlations[0, 1], mcca_als.predict_corr(test_view_1, test_view_2)[0, 1]))\n",
    "\n",
    "\"\"\"\n",
    "### Multiset PLS by alternating least squares\n",
    "\"\"\"\n",
    "mcca_pls = PLS(latent_dims=latent_dims, max_iter=max_iter)\n",
    "\n",
    "mcca_pls.fit(train_view_1, train_view_2,train_view_1)\n",
    "\n",
    "mcca_pls_results = np.stack(\n",
    "    (mcca_als.train_correlations[0, 1], mcca_pls.predict_corr(test_view_1, test_view_2)[0, 1]))"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPDHc24UonKG"
   },
   "source": [
    "# Tensor CCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TVFMkssXombY",
    "outputId": "ea4293a8-de44-4050-fc4d-8c440deaca27",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "\"\"\"\n",
    "### (Regularized) Tensor CCA(can pass more than 2 views)\n",
    "\"\"\"\n",
    "\n",
    "tcca = TCCA(latent_dims=latent_dims, c=c)\n",
    "\n",
    "tcca.fit(train_view_1[:,:100], train_view_2[:,:100],train_view_1[:,:100])\n",
    "\n",
    "tcca_results = np.stack((tcca.train_correlations[0, 1], tcca.predict_corr(test_view_1[:,:100], test_view_2[:,:100])[0, 1]))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "reconstruction error=1.0359315541660723\n",
      "iteration 1, reconstruction error: 0.983515436088197, decrease = 0.05241611807787527, unnormalized = 3.9147814852779983\n",
      "iteration 2, reconstruction error: 1.7819312951719173, decrease = -0.7984158590837203, unnormalized = 7.092793245952575\n",
      "iteration 3, reconstruction error: 1.1534564545338517, decrease = 0.6284748406380656, unnormalized = 4.591214135126797\n",
      "iteration 4, reconstruction error: 0.9551055092748623, decrease = 0.19835094525898944, unnormalized = 3.801698709547194\n",
      "iteration 5, reconstruction error: 1.0312838412555436, decrease = -0.07617833198068136, unnormalized = 4.104918682182772\n",
      "iteration 6, reconstruction error: 1.0480707167824341, decrease = -0.01678687552689051, unnormalized = 4.171737104239997\n",
      "iteration 7, reconstruction error: 0.9672468093769817, decrease = 0.08082390740545242, unnormalized = 3.850025899038014\n",
      "iteration 8, reconstruction error: 2.767246935789939, decrease = -1.8000001264129573, unnormalized = 11.014740259197374\n",
      "iteration 9, reconstruction error: 1.0817662438762101, decrease = 1.6854806919137288, unnormalized = 4.305858665288449\n",
      "iteration 10, reconstruction error: 1.0804182218586997, decrease = 0.0013480220175103863, unnormalized = 4.300493003050461\n",
      "iteration 11, reconstruction error: 1.080472765355278, decrease = -5.454349657818547e-05, unnormalized = 4.300710107798095\n",
      "iteration 12, reconstruction error: 1.0728083192231401, decrease = 0.007664446132137792, unnormalized = 4.270202572570847\n",
      "iteration 13, reconstruction error: 3.7929426322468567, decrease = -2.7201343130237166, unnormalized = 15.097415908894837\n",
      "iteration 14, reconstruction error: 1.0684368048705133, decrease = 2.724505827376343, unnormalized = 4.2528022117606925\n",
      "iteration 15, reconstruction error: 0.6529675938217542, decrease = 0.4154692110487591, unnormalized = 2.5990699820096137\n",
      "iteration 16, reconstruction error: 0.7146840675219369, decrease = -0.06171647370018274, unnormalized = 2.844726023300719\n",
      "iteration 17, reconstruction error: 0.6583346756722259, decrease = 0.056349391849710995, unnormalized = 2.620433096290531\n",
      "iteration 18, reconstruction error: 1.61336898448542, decrease = -0.9550343088131941, unnormalized = 6.421848399763093\n",
      "iteration 19, reconstruction error: 1.4825327236319281, decrease = 0.13083626085349187, unnormalized = 5.901068193577979\n",
      "iteration 20, reconstruction error: 1.2104110389792775, decrease = 0.27212168465265063, unnormalized = 4.817915968679576\n",
      "iteration 21, reconstruction error: 5.426181455247386, decrease = -4.215770416268109, unnormalized = 21.59835414607197\n",
      "iteration 22, reconstruction error: 0.9070775383281164, decrease = 4.51910391691927, unnormalized = 3.6105283378999413\n",
      "iteration 23, reconstruction error: 1.223935624464469, decrease = -0.3168580861363526, unnormalized = 4.871749182588319\n",
      "iteration 24, reconstruction error: 1.1176926282022746, decrease = 0.10624299626219447, unnormalized = 4.448859922850864\n",
      "iteration 25, reconstruction error: 1.0199204188719584, decrease = 0.09777220933031616, unnormalized = 4.059687754508076\n",
      "iteration 26, reconstruction error: 1.0013677928286502, decrease = 0.018552626043308207, unnormalized = 3.9858409451214296\n",
      "iteration 27, reconstruction error: 1.0093548117596605, decrease = -0.007987018931010281, unnormalized = 4.017632448016438\n",
      "iteration 28, reconstruction error: 1.011089296347677, decrease = -0.001734484588016505, unnormalized = 4.024536384551155\n",
      "iteration 29, reconstruction error: 1.1101044696633968, decrease = -0.09901517331571985, unnormalized = 4.418656042499474\n",
      "iteration 30, reconstruction error: 0.9628824072240443, decrease = 0.14722206243935254, unnormalized = 3.8326538476032312\n",
      "iteration 31, reconstruction error: 1.05870817729366, decrease = -0.09582577006961568, unnormalized = 4.214078415755507\n",
      "iteration 32, reconstruction error: 1.0611918211397022, decrease = -0.0024836438460422716, unnormalized = 4.223964303244149\n",
      "iteration 33, reconstruction error: 1.049618276484227, decrease = 0.01157354465547522, unnormalized = 4.177897005595525\n",
      "iteration 34, reconstruction error: 1.0496547120211648, decrease = -3.643553693777335e-05, unnormalized = 4.178042033482405\n",
      "iteration 35, reconstruction error: 1.0610475972761275, decrease = -0.011392885254962737, unnormalized = 4.2233902350697825\n",
      "iteration 36, reconstruction error: 1.0568709614597185, decrease = 0.004176635816409036, unnormalized = 4.206765568120111\n",
      "iteration 37, reconstruction error: 1.0480681681210966, decrease = 0.008802793338621884, unnormalized = 4.171726959557108\n",
      "iteration 38, reconstruction error: 1.0602433080010458, decrease = -0.012175139879949226, unnormalized = 4.220188844784114\n",
      "iteration 39, reconstruction error: 1.05665957621643, decrease = 0.0035837317846159067, unnormalized = 4.205924171019137\n",
      "iteration 40, reconstruction error: 1.0331442234423536, decrease = 0.023515352774076348, unnormalized = 4.112323741090065\n",
      "iteration 41, reconstruction error: 1.0251780049711776, decrease = 0.007966218471175956, unnormalized = 4.080615032274393\n",
      "iteration 42, reconstruction error: 1.0205728553296074, decrease = 0.0046051496415702164, unnormalized = 4.062284710357476\n",
      "iteration 43, reconstruction error: 1.023994152788478, decrease = -0.0034212974588705425, unnormalized = 4.075902831086609\n",
      "iteration 44, reconstruction error: 1.021804342164807, decrease = 0.0021898106236708514, unnormalized = 4.067186516353504\n",
      "iteration 45, reconstruction error: 1.0211273479706147, decrease = 0.0006769941941924262, unnormalized = 4.064491810973377\n",
      "iteration 46, reconstruction error: 1.021620194144298, decrease = -0.0004928461736832812, unnormalized = 4.066453534201127\n",
      "iteration 47, reconstruction error: 1.0212304877464795, decrease = 0.0003897063978184345, unnormalized = 4.064902348185235\n",
      "iteration 48, reconstruction error: 1.0210140475784386, decrease = 0.00021644016804089894, unnormalized = 4.064040830479029\n",
      "iteration 49, reconstruction error: 1.021169475584969, decrease = -0.00015542800653034483, unnormalized = 4.06465949558578\n",
      "iteration 50, reconstruction error: 1.021011761348919, decrease = 0.00015771423605004387, unnormalized = 4.064031730378852\n",
      "iteration 51, reconstruction error: 1.0209617084941607, decrease = 5.005285475823129e-05, unnormalized = 4.063832500166591\n",
      "iteration 52, reconstruction error: 1.0210318801773532, decrease = -7.017168319256584e-05, unnormalized = 4.0641118112948895\n",
      "iteration 53, reconstruction error: 1.0209913324965805, decrease = 4.0547680772773376e-05, unnormalized = 4.063950415444722\n",
      "iteration 54, reconstruction error: 1.0209756315275738, decrease = 1.570096900671558e-05, unnormalized = 4.063887919361272\n",
      "iteration 55, reconstruction error: 1.0209921048084314, decrease = -1.6473280857631778e-05, unnormalized = 4.063953489552178\n",
      "iteration 56, reconstruction error: 1.0209728538341465, decrease = 1.925097428490119e-05, unnormalized = 4.063876863039834\n",
      "iteration 57, reconstruction error: 1.0209711184230428, decrease = 1.735411103709339e-06, unnormalized = 4.0638699554154\n",
      "iteration 58, reconstruction error: 1.0209808107775655, decrease = -9.69235452275008e-06, unnormalized = 4.063908534830264\n",
      "iteration 59, reconstruction error: 1.020976389516343, decrease = 4.4212612224647785e-06, unnormalized = 4.063890936457183\n",
      "iteration 60, reconstruction error: 1.0209755319396376, decrease = 8.575767054264105e-07, unnormalized = 4.0638875229617915\n",
      "iteration 61, reconstruction error: 1.0209769266498652, decrease = -1.3947102275313483e-06, unnormalized = 4.063893074461621\n",
      "iteration 62, reconstruction error: 1.0209747579557171, decrease = 2.168694148041439e-06, unnormalized = 4.063884442198836\n",
      "iteration 63, reconstruction error: 1.020975885690233, decrease = -1.1277345157711949e-06, unnormalized = 4.063888931029453\n",
      "iteration 64, reconstruction error: 1.0209781146692412, decrease = -2.22897900825636e-06, unnormalized = 4.0638978032498905\n",
      "iteration 65, reconstruction error: 1.0209783855266847, decrease = -2.708574435850153e-07, unnormalized = 4.063898881369934\n",
      "iteration 66, reconstruction error: 1.0209788607802128, decrease = -4.752535280871939e-07, unnormalized = 4.063900773067456\n",
      "iteration 67, reconstruction error: 1.0209793719985334, decrease = -5.112183205646659e-07, unnormalized = 4.063902807919114\n",
      "iteration 68, reconstruction error: 1.020979720585982, decrease = -3.485874486841567e-07, unnormalized = 4.063904195435406\n",
      "iteration 69, reconstruction error: 1.0209807545851106, decrease = -1.0339991285412964e-06, unnormalized = 4.063908311162008\n",
      "iteration 70, reconstruction error: 1.0209819079659523, decrease = -1.1533808417230063e-06, unnormalized = 4.063912902075174\n",
      "iteration 71, reconstruction error: 1.0209827156337168, decrease = -8.07667764446407e-07, unnormalized = 4.063916116913187\n",
      "iteration 72, reconstruction error: 1.0209834347649327, decrease = -7.191312159093854e-07, unnormalized = 4.063918979340625\n",
      "iteration 73, reconstruction error: 1.020984127871251, decrease = -6.931063183390762e-07, unnormalized = 4.063921738178649\n",
      "iteration 74, reconstruction error: 1.0209848918901505, decrease = -7.640188994795238e-07, unnormalized = 4.063924779276869\n",
      "iteration 75, reconstruction error: 1.020985819565954, decrease = -9.276758035081656e-07, unnormalized = 4.063928471794472\n",
      "iteration 76, reconstruction error: 1.0209867606017635, decrease = -9.410358094985583e-07, unnormalized = 4.063932217490196\n",
      "iteration 77, reconstruction error: 1.0209876147180634, decrease = -8.541162999087248e-07, unnormalized = 4.0639356172118015\n",
      "iteration 78, reconstruction error: 1.0209884169665941, decrease = -8.02248530717975e-07, unnormalized = 4.063938810479115\n",
      "iteration 79, reconstruction error: 1.0209892128958582, decrease = -7.959292640524751e-07, unnormalized = 4.063941978593241\n",
      "iteration 80, reconstruction error: 1.0209900463821913, decrease = -8.33486333062794e-07, unnormalized = 4.063945296199397\n",
      "iteration 81, reconstruction error: 1.0209909217894482, decrease = -8.754072569061577e-07, unnormalized = 4.063948780667455\n",
      "iteration 82, reconstruction error: 1.020991794633728, decrease = -8.728442797423241e-07, unnormalized = 4.063952254933847\n",
      "iteration 83, reconstruction error: 1.0209926373758977, decrease = -8.427421698264936e-07, unnormalized = 4.063955609381904\n",
      "iteration 84, reconstruction error: 1.0209934577484774, decrease = -8.203725796462891e-07, unnormalized = 4.06395887479012\n",
      "iteration 85, reconstruction error: 1.0209942752474086, decrease = -8.174989312159653e-07, unnormalized = 4.063962128760076\n",
      "iteration 86, reconstruction error: 1.020995104118681, decrease = -8.288712725068592e-07, unnormalized = 4.06396542799646\n",
      "iteration 87, reconstruction error: 1.0209959414042389, decrease = -8.372855577754024e-07, unnormalized = 4.063968760725037\n",
      "iteration 88, reconstruction error: 1.0209967727625262, decrease = -8.313582873231695e-07, unnormalized = 4.063972069860727\n",
      "iteration 89, reconstruction error: 1.0209975899501156, decrease = -8.171875893747682e-07, unnormalized = 4.063975322591419\n",
      "iteration 90, reconstruction error: 1.0209983957348945, decrease = -8.057847789011419e-07, unnormalized = 4.063978529934403\n",
      "iteration 91, reconstruction error: 1.020999197298372, decrease = -8.015634775926372e-07, unnormalized = 4.063981720474933\n",
      "iteration 92, reconstruction error: 1.020999998835219, decrease = -8.015368468949902e-07, unnormalized = 4.063984910909463\n",
      "iteration 93, reconstruction error: 1.0210007983966265, decrease = -7.995614075273494e-07, unnormalized = 4.063988093480961\n",
      "iteration 94, reconstruction error: 1.0210015911589927, decrease = -7.927623661974081e-07, unnormalized = 4.063991248989577\n",
      "iteration 95, reconstruction error: 1.0210023746308172, decrease = -7.834718245280925e-07, unnormalized = 4.063994367518154\n",
      "iteration 96, reconstruction error: 1.0210031499603878, decrease = -7.753295705459351e-07, unnormalized = 4.063997453637331\n",
      "iteration 97, reconstruction error: 1.021003919635995, decrease = -7.696756072483879e-07, unnormalized = 4.064000517251491\n",
      "iteration 98, reconstruction error: 1.0210046848250725, decrease = -7.651890774340586e-07, unnormalized = 4.064003563007484\n",
      "iteration 99, reconstruction error: 1.0210054446197028, decrease = -7.597946303228298e-07, unnormalized = 4.064006587291437\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/ma/core.py:2832: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  order=order, subok=True, ndmin=ndmin)\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-J9QturUVUE"
   },
   "source": [
    "# Weighted GCCA/Missing Observation GCCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "58A4v1p_UVio"
   },
   "source": [
    "#observation_matrix\n",
    "K = np.ones((3, N))\n",
    "K[0, 200:] = 0\n",
    "K[1, :100] = 0\n",
    "\n",
    "#view weights\n",
    "view_weights=[1,2,1.2]\n",
    "\n",
    "c=[0.5,0.5,0.5]\n",
    "\n",
    "gcca = GCCA(latent_dims=latent_dims,c=c,view_weights=view_weights)\n",
    "\n",
    "gcca.fit(train_view_1, train_view_2,train_view_1,K=K)\n",
    "\n",
    "gcca_results = np.stack((gcca.train_correlations[0, 1], gcca.predict_corr(test_view_1, test_view_2)[0, 1]))"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9847WJ3liT6v"
   },
   "source": [
    "# Rgularised CCA solutions based on alternating minimisation/alternating least squares\n",
    "\n",
    "We implement Witten's penalized matrix decomposition form of sparse CCA using 'pmd'\n",
    "\n",
    "We implement Waaijenborg's penalized CCA using elastic net using 'elastic'\n",
    "\n",
    "We implement Mai's sparse CCA using 'scca'\n",
    "\n",
    "Furthermore, any of these methods can be extended to multiple views. Witten describes this method explicitly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ts_yQvzdhDQe",
    "outputId": "b56614dc-db37-4554-c5cc-6ddbb0e7ac45"
   },
   "source": [
    "\"\"\"\n",
    "### Ridge CCA (can pass more than 2 views)\n",
    "\"\"\"\n",
    "c1 = [0.1, 0.3, 0.7, 0.9]\n",
    "c2 = [0.1, 0.3, 0.7, 0.9]\n",
    "param_candidates = {'c': list(itertools.product(c1, c2))}\n",
    "\n",
    "ridge = rCCA(latent_dims=latent_dims).gridsearch_fit(\n",
    "    train_view_1,\n",
    "    train_view_2,\n",
    "    param_candidates=param_candidates,\n",
    "    folds=cv_folds,\n",
    "    verbose=True, jobs=jobs,\n",
    "    plot=True)\n",
    "\n",
    "ridge_results = np.stack((ridge.train_correlations[0, 1, :], ridge.predict_corr(test_view_1, test_view_2)[0, 1, :]))\n",
    "\n",
    "\"\"\"\n",
    "### Sparse CCA (Penalized Matrix Decomposition) (can pass more than 2 views)\n",
    "\"\"\"\n",
    "\n",
    "# PMD\n",
    "c1 = [1, 3, 7, 9]\n",
    "c2 = [1, 3, 7, 9]\n",
    "param_candidates = {'c': list(itertools.product(c1, c2))}\n",
    "\n",
    "pmd = PMD(latent_dims=latent_dims, max_iter=max_iter).gridsearch_fit(\n",
    "    train_view_1,\n",
    "    train_view_2,\n",
    "    param_candidates=param_candidates,\n",
    "    folds=cv_folds,\n",
    "    verbose=True, jobs=jobs,\n",
    "    plot=True)\n",
    "\n",
    "pmd_results = np.stack((pmd.train_correlations[0, 1, :], pmd.predict_corr(test_view_1, test_view_2)[0, 1, :]))\n",
    "\n",
    "\"\"\"\n",
    "### Sparse CCA (can pass more than 2 views)\n",
    "\"\"\"\n",
    "\n",
    "# Sparse CCA\n",
    "c1 = [0.00001, 0.0001]\n",
    "c2 = [0.00001, 0.0001]\n",
    "param_candidates = {'c': list(itertools.product(c1, c2))}\n",
    "\n",
    "scca = SCCA(latent_dims=latent_dims, max_iter=max_iter).gridsearch_fit(\n",
    "    train_view_1,\n",
    "    train_view_2,\n",
    "    param_candidates=param_candidates,\n",
    "    folds=cv_folds,\n",
    "    verbose=True,\n",
    "    jobs=jobs, plot=True)\n",
    "\n",
    "scca_results = np.stack(\n",
    "    (scca.train_correlations[0, 1, :], scca.predict_corr(test_view_1, test_view_2)[0, 1, :]))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "### Elastic CCA (can pass more than 2 views)\n",
    "\"\"\"\n",
    "\n",
    "# Elastic CCA\n",
    "c1 = [0.001, 0.0001]\n",
    "c2 = [0.001, 0.0001]\n",
    "l1_1 = [0.01, 0.1]\n",
    "l1_2 = [0.01, 0.1]\n",
    "param_candidates = {'c': list(itertools.product(c1, c2)), 'l1_ratio': list(itertools.product(l1_1, l1_2))}\n",
    "\n",
    "elastic = ElasticCCA(latent_dims=latent_dims,\n",
    "                              max_iter=max_iter).gridsearch_fit(train_view_1,\n",
    "                                                                train_view_2,\n",
    "                                                                param_candidates=param_candidates,\n",
    "                                                                folds=cv_folds,\n",
    "                                                                verbose=True,\n",
    "                                                                jobs=jobs,\n",
    "                                                                plot=True)\n",
    "\n",
    "elastic_results = np.stack(\n",
    "    (elastic.train_correlations[0, 1, :], elastic.predict_corr(test_view_1, test_view_2)[0, 1, :]))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.5014489575372487\n",
      "Standard deviation :  0.08764675842178948\n",
      "{'c': (0.9, 0.9)}\n",
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.2606814145625667\n",
      "Standard deviation :  0.08007310642587495\n",
      "{'c': (9, 9)}\n",
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.0626303688460361\n",
      "Standard deviation :  0.1260582986894677\n",
      "{'c': (0.0001, 1e-05)}\n",
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.0063428646485044\n",
      "Standard deviation :  0.15833033841844305\n",
      "{'c': (0.001, 0.0001), 'l1_ratio': (0.1, 0.1)}\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaHqF5CljrCb"
   },
   "source": [
    "# Kernel CCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtH38KO8hLFL",
    "outputId": "6fcc0aab-8f2d-47bc-cb75-96848b44b38e"
   },
   "source": [
    "\"\"\"\n",
    "### Kernel CCA\n",
    "\n",
    "Similarly, we can use kernel CCA methods with [method='kernel']\n",
    "\n",
    "We can use different kernels and their associated parameters in a similar manner to before\n",
    "- regularized linear kernel CCA: parameters :  'kernel'='linear', 0<'c'<1\n",
    "- polynomial kernel CCA: parameters : 'kernel'='poly', 'degree', 0<'c'<1\n",
    "- gaussian rbf kernel CCA: parameters : 'kernel'='gaussian', 'sigma', 0<'c'<1\n",
    "\"\"\"\n",
    "# %%\n",
    "# r-kernel cca\n",
    "c1 = [0.9, 0.99]\n",
    "c2 = [0.9, 0.99]\n",
    "\n",
    "param_candidates = {'kernel': ['linear', 'linear'], 'c': list(itertools.product(c1, c2))}\n",
    "\n",
    "kernel_reg = KCCA(latent_dims=latent_dims).gridsearch_fit(train_view_1, train_view_2,\n",
    "                                                          folds=cv_folds,\n",
    "                                                          param_candidates=param_candidates,\n",
    "                                                          verbose=True, jobs=jobs,\n",
    "                                                          plot=True)\n",
    "kernel_reg_results = np.stack((\n",
    "    kernel_reg.train_correlations[0, 1, :],\n",
    "    kernel_reg.predict_corr(test_view_1, test_view_2)[0, 1, :]))\n",
    "\n",
    "# kernel cca (poly)\n",
    "degree1 = [2, 3]\n",
    "degree2 = [2, 3]\n",
    "\n",
    "param_candidates = {'kernel': ['poly', 'poly'], 'degree': list(itertools.product(degree1, degree2)),\n",
    "                    'c': list(itertools.product(c1, c2))}\n",
    "\n",
    "kernel_poly = KCCA(latent_dims=latent_dims).gridsearch_fit(train_view_1, train_view_2,\n",
    "                                                           folds=cv_folds,\n",
    "                                                           param_candidates=param_candidates,\n",
    "                                                           verbose=True, jobs=jobs,\n",
    "                                                           plot=True)\n",
    "\n",
    "kernel_poly_results = np.stack((\n",
    "    kernel_poly.train_correlations[0, 1, :],\n",
    "    kernel_poly.predict_corr(test_view_1, test_view_2)[0, 1, :]))\n",
    "\n",
    "# kernel cca (gaussian)\n",
    "gamma1 = [1e+1, 1e+2, 1e+3]\n",
    "gamma2 = [1e+1, 1e+2, 1e+3]\n",
    "\n",
    "param_candidates = {'kernel': ['rbf', 'rbf'], 'sigma': list(itertools.product(gamma1, gamma2)),\n",
    "                    'c': list(itertools.product(c1, c2))}\n",
    "\n",
    "kernel_gaussian = KCCA(latent_dims=latent_dims).gridsearch_fit(train_view_1, train_view_2,\n",
    "                                                               folds=cv_folds,\n",
    "                                                               param_candidates=param_candidates,\n",
    "                                                               verbose=True, jobs=jobs,\n",
    "                                                               plot=True)\n",
    "\n",
    "kernel_gaussian_results = np.stack((\n",
    "    kernel_gaussian.train_correlations[0, 1, :],\n",
    "    kernel_gaussian.predict_corr(test_view_1, test_view_2)[0, 1, :]))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.5644372549788443\n",
      "Standard deviation :  0.04188014522341969\n",
      "{'kernel': 'linear', 'c': (0.99, 0.99)}\n",
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.0926074055634412\n",
      "Standard deviation :  0.07267374745648462\n",
      "{'kernel': 'poly', 'degree': 3, 'c': (0.9, 0.9)}\n",
      "cross validation\n",
      "number of folds:  5\n",
      "Best score :  1.2525067748983651\n",
      "Standard deviation :  0.11774870350074017\n",
      "{'kernel': 'rbf', 'sigma': 100.0, 'c': (0.9, 0.9)}\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ltKrtIkICK"
   },
   "source": [
    "# Deep CCA\n",
    "\n",
    "DCCA can be optimized using Andrew's original tracenorm objective or Wang's DCCA by nonlinear orthogonal iterations using the argument als=True."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0nTlPSEhVeP",
    "outputId": "60ef2c80-b99e-4718-9df3-46c4612936d1"
   },
   "source": [
    "\"\"\"\n",
    "### Deep Learning\n",
    "\n",
    "We also have deep CCA methods (and autoencoder variants)\n",
    "- Deep CCA (DCCA)\n",
    "- Deep Canonically Correlated Autoencoders (DCCAE)\n",
    "\n",
    "We introduce a Config class from configuration.py. This contains a number of default settings for running DCCA.\n",
    "\n",
    "\"\"\"\n",
    "from cca_zoo import deepwrapper, objectives, dcca, deep_models\n",
    "\n",
    "# %%\n",
    "# DCCA\n",
    "print('DCCA')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "dcca_model = DCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2])\n",
    "\n",
    "dcca_model = deepwrapper.DeepWrapper(dcca_model)\n",
    "\n",
    "dcca_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dcca_results = np.stack((dcca_model.train_correlations[0, 1], dcca_model.predict_corr(test_dataset)[0, 1]))\n",
    "\n",
    "\n",
    "from cca_zoo import dcca_noi\n",
    "# DCCA_NOI\n",
    "# Note that als=True\n",
    "print('DCCA by non-linear orthogonal iterations')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "dcca_noi_model = dcca_noi.DCCA_NOI(latent_dims=latent_dims, encoders=[encoder_1, encoder_2])\n",
    "\n",
    "dcca_noi_model = deepwrapper.DeepWrapper(dcca_noi_model)\n",
    "\n",
    "dcca_noi_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dcca_noi_results = np.stack(\n",
    "    (dcca_noi_model.train_correlations[0, 1], dcca_noi_model.predict_corr(test_dataset)[0, 1]))"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "DCCA\n",
      "total parameters:  201476\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average train loss: -0.2801\n",
      "====> Epoch: 1 Average val loss: -0.7396\n",
      "Min loss -0.74\n",
      "====> Epoch: 2 Average train loss: -0.6931\n",
      "====> Epoch: 2 Average val loss: -0.9382\n",
      "Min loss -0.94\n",
      "====> Epoch: 3 Average train loss: -0.9064\n",
      "====> Epoch: 3 Average val loss: -1.0791\n",
      "Min loss -1.08\n",
      "====> Epoch: 4 Average train loss: -1.1217\n",
      "====> Epoch: 4 Average val loss: -1.1549\n",
      "Min loss -1.15\n",
      "====> Epoch: 5 Average train loss: -1.1235\n",
      "====> Epoch: 5 Average val loss: -1.2832\n",
      "Min loss -1.28\n",
      "====> Epoch: 6 Average train loss: -1.1602\n",
      "====> Epoch: 6 Average val loss: -1.1615\n",
      "====> Epoch: 7 Average train loss: -1.2731\n",
      "====> Epoch: 7 Average val loss: -1.2787\n",
      "====> Epoch: 8 Average train loss: -1.2215\n",
      "====> Epoch: 8 Average val loss: -1.2810\n",
      "====> Epoch: 9 Average train loss: -1.2851\n",
      "====> Epoch: 9 Average val loss: -1.2833\n",
      "Min loss -1.28\n",
      "====> Epoch: 10 Average train loss: -1.2556\n",
      "====> Epoch: 10 Average val loss: -1.2792\n",
      "====> Epoch: 11 Average train loss: -1.2700\n",
      "====> Epoch: 11 Average val loss: -1.2791\n",
      "====> Epoch: 12 Average train loss: -1.3101\n",
      "====> Epoch: 12 Average val loss: -1.3831\n",
      "Min loss -1.38\n",
      "====> Epoch: 13 Average train loss: -1.3162\n",
      "====> Epoch: 13 Average val loss: -1.3281\n",
      "====> Epoch: 14 Average train loss: -1.3311\n",
      "====> Epoch: 14 Average val loss: -1.3988\n",
      "Min loss -1.40\n",
      "====> Epoch: 15 Average train loss: -1.3490\n",
      "====> Epoch: 15 Average val loss: -1.3632\n",
      "====> Epoch: 16 Average train loss: -1.3517\n",
      "====> Epoch: 16 Average val loss: -1.3115\n",
      "====> Epoch: 17 Average train loss: -1.3702\n",
      "====> Epoch: 17 Average val loss: -1.3271\n",
      "====> Epoch: 18 Average train loss: -1.3715\n",
      "====> Epoch: 18 Average val loss: -1.3526\n",
      "====> Epoch: 19 Average train loss: -1.3391\n",
      "====> Epoch: 19 Average val loss: -1.3820\n",
      "====> Epoch: 20 Average train loss: -1.3713\n",
      "====> Epoch: 20 Average val loss: -1.3903\n",
      "====> Epoch: 21 Average train loss: -1.3808\n",
      "====> Epoch: 21 Average val loss: -1.3985\n",
      "====> Epoch: 22 Average train loss: -1.4191\n",
      "====> Epoch: 22 Average val loss: -1.4267\n",
      "Min loss -1.43\n",
      "====> Epoch: 23 Average train loss: -1.4052\n",
      "====> Epoch: 23 Average val loss: -1.3864\n",
      "====> Epoch: 24 Average train loss: -1.4040\n",
      "====> Epoch: 24 Average val loss: -1.3631\n",
      "====> Epoch: 25 Average train loss: -1.4483\n",
      "====> Epoch: 25 Average val loss: -1.3924\n",
      "====> Epoch: 26 Average train loss: -1.4502\n",
      "====> Epoch: 26 Average val loss: -1.3634\n",
      "====> Epoch: 27 Average train loss: -1.3572\n",
      "====> Epoch: 27 Average val loss: -1.4598\n",
      "Min loss -1.46\n",
      "====> Epoch: 28 Average train loss: -1.4412\n",
      "====> Epoch: 28 Average val loss: -1.4584\n",
      "====> Epoch: 29 Average train loss: -1.4683\n",
      "====> Epoch: 29 Average val loss: -1.4510\n",
      "====> Epoch: 30 Average train loss: -1.4310\n",
      "====> Epoch: 30 Average val loss: -1.4532\n",
      "====> Epoch: 31 Average train loss: -1.4624\n",
      "====> Epoch: 31 Average val loss: -1.4488\n",
      "====> Epoch: 32 Average train loss: -1.4499\n",
      "====> Epoch: 32 Average val loss: -1.4860\n",
      "Min loss -1.49\n",
      "====> Epoch: 33 Average train loss: -1.4464\n",
      "====> Epoch: 33 Average val loss: -1.4686\n",
      "====> Epoch: 34 Average train loss: -1.4898\n",
      "====> Epoch: 34 Average val loss: -1.4458\n",
      "====> Epoch: 35 Average train loss: -1.4351\n",
      "====> Epoch: 35 Average val loss: -1.4659\n",
      "====> Epoch: 36 Average train loss: -1.4502\n",
      "====> Epoch: 36 Average val loss: -1.4747\n",
      "====> Epoch: 37 Average train loss: -1.4252\n",
      "====> Epoch: 37 Average val loss: -1.4754\n",
      "====> Epoch: 38 Average train loss: -1.5401\n",
      "====> Epoch: 38 Average val loss: -1.4627\n",
      "====> Epoch: 39 Average train loss: -1.4939\n",
      "====> Epoch: 39 Average val loss: -1.4712\n",
      "====> Epoch: 40 Average train loss: -1.4386\n",
      "====> Epoch: 40 Average val loss: -1.4960\n",
      "Min loss -1.50\n",
      "====> Epoch: 41 Average train loss: -1.5071\n",
      "====> Epoch: 41 Average val loss: -1.4687\n",
      "====> Epoch: 42 Average train loss: -1.5088\n",
      "====> Epoch: 42 Average val loss: -1.4866\n",
      "====> Epoch: 43 Average train loss: -1.5576\n",
      "====> Epoch: 43 Average val loss: -1.4883\n",
      "====> Epoch: 44 Average train loss: -1.5516\n",
      "====> Epoch: 44 Average val loss: -1.5537\n",
      "Min loss -1.55\n",
      "====> Epoch: 45 Average train loss: -1.5591\n",
      "====> Epoch: 45 Average val loss: -1.5324\n",
      "====> Epoch: 46 Average train loss: -1.5378\n",
      "====> Epoch: 46 Average val loss: -1.5396\n",
      "====> Epoch: 47 Average train loss: -1.5817\n",
      "====> Epoch: 47 Average val loss: -1.5504\n",
      "====> Epoch: 48 Average train loss: -1.5328\n",
      "====> Epoch: 48 Average val loss: -1.5511\n",
      "====> Epoch: 49 Average train loss: -1.5964\n",
      "====> Epoch: 49 Average val loss: -1.5404\n",
      "====> Epoch: 50 Average train loss: -1.5683\n",
      "====> Epoch: 50 Average val loss: -1.5228\n",
      "DCCA by non-linear orthogonal iterations\n",
      "total parameters:  201476\n",
      "====> Epoch: 1 Average train loss: -0.3063\n",
      "====> Epoch: 1 Average val loss: -0.4249\n",
      "Min loss -0.42\n",
      "====> Epoch: 2 Average train loss: -0.4503\n",
      "====> Epoch: 2 Average val loss: -0.4881\n",
      "Min loss -0.49\n",
      "====> Epoch: 3 Average train loss: -0.3723\n",
      "====> Epoch: 3 Average val loss: -0.1837\n",
      "====> Epoch: 4 Average train loss: -0.2049\n",
      "====> Epoch: 4 Average val loss: -0.0564\n",
      "====> Epoch: 5 Average train loss: -0.1045\n",
      "====> Epoch: 5 Average val loss: -0.1627\n",
      "====> Epoch: 6 Average train loss: -0.1226\n",
      "====> Epoch: 6 Average val loss: -0.2858\n",
      "====> Epoch: 7 Average train loss: -0.2620\n",
      "====> Epoch: 7 Average val loss: -0.2900\n",
      "====> Epoch: 8 Average train loss: -0.2653\n",
      "====> Epoch: 8 Average val loss: -0.2041\n",
      "====> Epoch: 9 Average train loss: -0.2492\n",
      "====> Epoch: 9 Average val loss: -0.1994\n",
      "====> Epoch: 10 Average train loss: -0.1833\n",
      "====> Epoch: 10 Average val loss: -0.1259\n",
      "====> Epoch: 11 Average train loss: -0.0884\n",
      "====> Epoch: 11 Average val loss: -0.0673\n",
      "====> Epoch: 12 Average train loss: -0.0962\n",
      "====> Epoch: 12 Average val loss: -0.0805\n",
      "====> Epoch: 13 Average train loss: -0.0877\n",
      "====> Epoch: 13 Average val loss: -0.0952\n",
      "====> Epoch: 14 Average train loss: -0.0953\n",
      "====> Epoch: 14 Average val loss: -0.1018\n",
      "====> Epoch: 15 Average train loss: -0.0914\n",
      "====> Epoch: 15 Average val loss: -0.0942\n",
      "====> Epoch: 16 Average train loss: -0.1042\n",
      "====> Epoch: 16 Average val loss: -0.1027\n",
      "====> Epoch: 17 Average train loss: -0.1217\n",
      "====> Epoch: 17 Average val loss: -0.0939\n",
      "====> Epoch: 18 Average train loss: -0.1093\n",
      "====> Epoch: 18 Average val loss: -0.0687\n",
      "====> Epoch: 19 Average train loss: -0.0721\n",
      "====> Epoch: 19 Average val loss: -0.0730\n",
      "====> Epoch: 20 Average train loss: -0.0898\n",
      "====> Epoch: 20 Average val loss: -0.0691\n",
      "====> Epoch: 21 Average train loss: -0.0599\n",
      "====> Epoch: 21 Average val loss: -0.0570\n",
      "====> Epoch: 22 Average train loss: -0.0771\n",
      "====> Epoch: 22 Average val loss: -0.0427\n",
      "====> Epoch: 23 Average train loss: -0.0688\n",
      "====> Epoch: 23 Average val loss: -0.0507\n",
      "====> Epoch: 24 Average train loss: -0.0490\n",
      "====> Epoch: 24 Average val loss: -0.0307\n",
      "====> Epoch: 25 Average train loss: -0.0418\n",
      "====> Epoch: 25 Average val loss: -0.0336\n",
      "====> Epoch: 26 Average train loss: -0.0404\n",
      "====> Epoch: 26 Average val loss: -0.0287\n",
      "====> Epoch: 27 Average train loss: -0.0340\n",
      "====> Epoch: 27 Average val loss: -0.0123\n",
      "====> Epoch: 28 Average train loss: -0.0275\n",
      "====> Epoch: 28 Average val loss: -0.0214\n",
      "====> Epoch: 29 Average train loss: -0.0150\n",
      "====> Epoch: 29 Average val loss: -0.0265\n",
      "====> Epoch: 30 Average train loss: -0.0302\n",
      "====> Epoch: 30 Average val loss: -0.0179\n",
      "====> Epoch: 31 Average train loss: -0.0195\n",
      "====> Epoch: 31 Average val loss: -0.0124\n",
      "====> Epoch: 32 Average train loss: -0.0133\n",
      "====> Epoch: 32 Average val loss: -0.0055\n",
      "====> Epoch: 33 Average train loss: -0.0141\n",
      "====> Epoch: 33 Average val loss: -0.0051\n",
      "====> Epoch: 34 Average train loss: -0.0103\n",
      "====> Epoch: 34 Average val loss: -0.0073\n",
      "====> Epoch: 35 Average train loss: -0.0065\n",
      "====> Epoch: 35 Average val loss: -0.0077\n",
      "====> Epoch: 36 Average train loss: -0.0074\n",
      "====> Epoch: 36 Average val loss: -0.0059\n",
      "====> Epoch: 37 Average train loss: -0.0047\n",
      "====> Epoch: 37 Average val loss: -0.0080\n",
      "====> Epoch: 38 Average train loss: -0.0050\n",
      "====> Epoch: 38 Average val loss: -0.0056\n",
      "====> Epoch: 39 Average train loss: -0.0035\n",
      "====> Epoch: 39 Average val loss: -0.0072\n",
      "====> Epoch: 40 Average train loss: -0.0049\n",
      "====> Epoch: 40 Average val loss: -0.0063\n",
      "====> Epoch: 41 Average train loss: -0.0045\n",
      "====> Epoch: 41 Average val loss: -0.0058\n",
      "====> Epoch: 42 Average train loss: -0.0035\n",
      "====> Epoch: 42 Average val loss: -0.0050\n",
      "====> Epoch: 43 Average train loss: -0.0045\n",
      "====> Epoch: 43 Average val loss: -0.0056\n",
      "====> Epoch: 44 Average train loss: -0.0048\n",
      "====> Epoch: 44 Average val loss: -0.0055\n",
      "====> Epoch: 45 Average train loss: -0.0037\n",
      "====> Epoch: 45 Average val loss: -0.0044\n",
      "====> Epoch: 46 Average train loss: -0.0056\n",
      "====> Epoch: 46 Average val loss: -0.0054\n",
      "====> Epoch: 47 Average train loss: -0.0051\n",
      "====> Epoch: 47 Average val loss: -0.0047\n",
      "====> Epoch: 48 Average train loss: -0.0051\n",
      "====> Epoch: 48 Average val loss: -0.0052\n",
      "====> Epoch: 49 Average train loss: -0.0058\n",
      "====> Epoch: 49 Average val loss: -0.0069\n",
      "====> Epoch: 50 Average train loss: -0.0056\n",
      "====> Epoch: 50 Average val loss: -0.0053\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1_ce5uMJUJ8"
   },
   "source": [
    "# DCCA with custom optimizers and schedulers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGC3HS2YJUk1",
    "outputId": "40791dd4-d2cf-40e0-dda4-a2ac278f463e"
   },
   "source": [
    "# DCCA\n",
    "optimizers = [optim.Adam(encoder_1.parameters(), lr=1e-4), optim.Adam(encoder_2.parameters(), lr=1e-4)]\n",
    "schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizers[0], 1),\n",
    "              optim.lr_scheduler.ReduceLROnPlateau(optimizers[1])]\n",
    "dcca_model = dcca.DCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2],\n",
    "                                objective=objectives.CCA, optimizers=optimizers, schedulers=schedulers)\n",
    "# hidden_layer_sizes are shown explicitly but these are also the defaults\n",
    "dcca_model = deepwrapper.DeepWrapper(dcca_model)\n",
    "dcca_model.fit(train_dataset, val_dataset=val_dataset,epochs=20)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "total parameters:  201476\n",
      "====> Epoch: 1 Average train loss: -0.0069\n",
      "====> Epoch: 1 Average val loss: -0.0096\n",
      "Min loss -0.01\n",
      "====> Epoch: 2 Average train loss: -0.0098\n",
      "====> Epoch: 2 Average val loss: -0.0104\n",
      "Min loss -0.01\n",
      "====> Epoch: 3 Average train loss: -0.0113\n",
      "====> Epoch: 3 Average val loss: -0.0143\n",
      "Min loss -0.01\n",
      "====> Epoch: 4 Average train loss: -0.0127\n",
      "====> Epoch: 4 Average val loss: -0.0165\n",
      "Min loss -0.02\n",
      "====> Epoch: 5 Average train loss: -0.0164\n",
      "====> Epoch: 5 Average val loss: -0.0204\n",
      "Min loss -0.02\n",
      "====> Epoch: 6 Average train loss: -0.0207\n",
      "====> Epoch: 6 Average val loss: -0.0222\n",
      "Min loss -0.02\n",
      "====> Epoch: 7 Average train loss: -0.0256\n",
      "====> Epoch: 7 Average val loss: -0.0270\n",
      "Min loss -0.03\n",
      "====> Epoch: 8 Average train loss: -0.0264\n",
      "====> Epoch: 8 Average val loss: -0.0319\n",
      "Min loss -0.03\n",
      "====> Epoch: 9 Average train loss: -0.0345\n",
      "====> Epoch: 9 Average val loss: -0.0286\n",
      "====> Epoch: 10 Average train loss: -0.0320\n",
      "====> Epoch: 10 Average val loss: -0.0386\n",
      "Min loss -0.04\n",
      "====> Epoch: 11 Average train loss: -0.0406\n",
      "====> Epoch: 11 Average val loss: -0.0357\n",
      "====> Epoch: 12 Average train loss: -0.0457\n",
      "====> Epoch: 12 Average val loss: -0.0462\n",
      "Min loss -0.05\n",
      "====> Epoch: 13 Average train loss: -0.0511\n",
      "====> Epoch: 13 Average val loss: -0.0597\n",
      "Min loss -0.06\n",
      "====> Epoch: 14 Average train loss: -0.0565\n",
      "====> Epoch: 14 Average val loss: -0.0587\n",
      "====> Epoch: 15 Average train loss: -0.0631\n",
      "====> Epoch: 15 Average val loss: -0.0615\n",
      "Min loss -0.06\n",
      "====> Epoch: 16 Average train loss: -0.0666\n",
      "====> Epoch: 16 Average val loss: -0.0611\n",
      "====> Epoch: 17 Average train loss: -0.0724\n",
      "====> Epoch: 17 Average val loss: -0.0654\n",
      "Min loss -0.07\n",
      "====> Epoch: 18 Average train loss: -0.0647\n",
      "====> Epoch: 18 Average val loss: -0.0664\n",
      "Min loss -0.07\n",
      "====> Epoch: 19 Average train loss: -0.0738\n",
      "====> Epoch: 19 Average val loss: -0.0960\n",
      "Min loss -0.10\n",
      "====> Epoch: 20 Average train loss: -0.0878\n",
      "====> Epoch: 20 Average val loss: -0.0818\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeepWrapper(device='cpu',\n",
       "            model=DCCA(\n",
       "  (encoders): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (layers): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "),\n",
       "            tensorboard=False, tensorboard_tag=None)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NZ5ZB9cNx-o"
   },
   "source": [
    "# DGCCA and DMCCA for more than 2 views\n",
    "\n",
    "The only change we need to make is to the objective argument to perform DGCCA and DMCCA."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAClfNMYNxq4",
    "outputId": "6fff1efc-16dc-4f3d-ea4b-bead3a518b5d"
   },
   "source": [
    "# DGCCA\n",
    "print('DGCCA')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "dgcca_model = dcca.DCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2], objective=objectives.GCCA)\n",
    "\n",
    "dgcca_model = deepwrapper.DeepWrapper(dgcca_model)\n",
    "\n",
    "dgcca_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dgcca_results = np.stack(\n",
    "    (dgcca_model.train_correlations[0, 1], dgcca_model.predict_corr(test_dataset)[0, 1]))\n",
    "\n",
    "# DMCCA\n",
    "print('DMCCA')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "dmcca_model = dcca.DCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2], objective=objectives.MCCA)\n",
    "\n",
    "dmcca_model = deepwrapper.DeepWrapper(dmcca_model)\n",
    "\n",
    "dmcca_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dmcca_results = np.stack(\n",
    "    (dmcca_model.train_correlations[0, 1], dmcca_model.predict_corr(test_dataset)[0, 1]))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "DGCCA\n",
      "total parameters:  201476\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average train loss: -0.0942\n",
      "====> Epoch: 1 Average val loss: -0.7712\n",
      "Min loss -0.77\n",
      "====> Epoch: 2 Average train loss: -0.9046\n",
      "====> Epoch: 2 Average val loss: -1.1785\n",
      "Min loss -1.18\n",
      "====> Epoch: 3 Average train loss: -1.1812\n",
      "====> Epoch: 3 Average val loss: -1.2140\n",
      "Min loss -1.21\n",
      "====> Epoch: 4 Average train loss: -1.2060\n",
      "====> Epoch: 4 Average val loss: -1.2871\n",
      "Min loss -1.29\n",
      "====> Epoch: 5 Average train loss: -1.2845\n",
      "====> Epoch: 5 Average val loss: -1.3248\n",
      "Min loss -1.32\n",
      "====> Epoch: 6 Average train loss: -1.3221\n",
      "====> Epoch: 6 Average val loss: -1.3468\n",
      "Min loss -1.35\n",
      "====> Epoch: 7 Average train loss: -1.3360\n",
      "====> Epoch: 7 Average val loss: -1.3763\n",
      "Min loss -1.38\n",
      "====> Epoch: 8 Average train loss: -1.3667\n",
      "====> Epoch: 8 Average val loss: -1.2920\n",
      "====> Epoch: 9 Average train loss: -1.3541\n",
      "====> Epoch: 9 Average val loss: -1.3590\n",
      "====> Epoch: 10 Average train loss: -1.3587\n",
      "====> Epoch: 10 Average val loss: -1.3665\n",
      "====> Epoch: 11 Average train loss: -1.4102\n",
      "====> Epoch: 11 Average val loss: -1.3375\n",
      "====> Epoch: 12 Average train loss: -1.3591\n",
      "====> Epoch: 12 Average val loss: -1.3908\n",
      "Min loss -1.39\n",
      "====> Epoch: 13 Average train loss: -1.4381\n",
      "====> Epoch: 13 Average val loss: -1.3869\n",
      "====> Epoch: 14 Average train loss: -1.4021\n",
      "====> Epoch: 14 Average val loss: -1.4238\n",
      "Min loss -1.42\n",
      "====> Epoch: 15 Average train loss: -1.4460\n",
      "====> Epoch: 15 Average val loss: -1.4079\n",
      "====> Epoch: 16 Average train loss: -1.4229\n",
      "====> Epoch: 16 Average val loss: -1.4414\n",
      "Min loss -1.44\n",
      "====> Epoch: 17 Average train loss: -1.4202\n",
      "====> Epoch: 17 Average val loss: -1.4080\n",
      "====> Epoch: 18 Average train loss: -1.4583\n",
      "====> Epoch: 18 Average val loss: -1.4513\n",
      "Min loss -1.45\n",
      "====> Epoch: 19 Average train loss: -1.4252\n",
      "====> Epoch: 19 Average val loss: -1.3602\n",
      "====> Epoch: 20 Average train loss: -1.4559\n",
      "====> Epoch: 20 Average val loss: -1.4479\n",
      "====> Epoch: 21 Average train loss: -1.5370\n",
      "====> Epoch: 21 Average val loss: -1.4409\n",
      "====> Epoch: 22 Average train loss: -1.5038\n",
      "====> Epoch: 22 Average val loss: -1.4957\n",
      "Min loss -1.50\n",
      "====> Epoch: 23 Average train loss: -1.4823\n",
      "====> Epoch: 23 Average val loss: -1.5260\n",
      "Min loss -1.53\n",
      "====> Epoch: 24 Average train loss: -1.5121\n",
      "====> Epoch: 24 Average val loss: -1.4705\n",
      "====> Epoch: 25 Average train loss: -1.5139\n",
      "====> Epoch: 25 Average val loss: -1.4685\n",
      "====> Epoch: 26 Average train loss: -1.5354\n",
      "====> Epoch: 26 Average val loss: -1.4958\n",
      "====> Epoch: 27 Average train loss: -1.5582\n",
      "====> Epoch: 27 Average val loss: -1.5103\n",
      "====> Epoch: 28 Average train loss: -1.5220\n",
      "====> Epoch: 28 Average val loss: -1.5138\n",
      "====> Epoch: 29 Average train loss: -1.5641\n",
      "====> Epoch: 29 Average val loss: -1.5009\n",
      "====> Epoch: 30 Average train loss: -1.5915\n",
      "====> Epoch: 30 Average val loss: -1.4514\n",
      "====> Epoch: 31 Average train loss: -1.6001\n",
      "====> Epoch: 31 Average val loss: -1.5433\n",
      "Min loss -1.54\n",
      "====> Epoch: 32 Average train loss: -1.5778\n",
      "====> Epoch: 32 Average val loss: -1.5339\n",
      "====> Epoch: 33 Average train loss: -1.6123\n",
      "====> Epoch: 33 Average val loss: -1.5046\n",
      "====> Epoch: 34 Average train loss: -1.5899\n",
      "====> Epoch: 34 Average val loss: -1.5291\n",
      "====> Epoch: 35 Average train loss: -1.5703\n",
      "====> Epoch: 35 Average val loss: -1.5110\n",
      "====> Epoch: 36 Average train loss: -1.6124\n",
      "====> Epoch: 36 Average val loss: -1.5566\n",
      "Min loss -1.56\n",
      "====> Epoch: 37 Average train loss: -1.6387\n",
      "====> Epoch: 37 Average val loss: -1.5655\n",
      "Min loss -1.57\n",
      "====> Epoch: 38 Average train loss: -1.5949\n",
      "====> Epoch: 38 Average val loss: -1.5457\n",
      "====> Epoch: 39 Average train loss: -1.5920\n",
      "====> Epoch: 39 Average val loss: -1.5484\n",
      "====> Epoch: 40 Average train loss: -1.6304\n",
      "====> Epoch: 40 Average val loss: -1.5543\n",
      "====> Epoch: 41 Average train loss: -1.6371\n",
      "====> Epoch: 41 Average val loss: -1.5629\n",
      "====> Epoch: 42 Average train loss: -1.6256\n",
      "====> Epoch: 42 Average val loss: -1.5308\n",
      "====> Epoch: 43 Average train loss: -1.6408\n",
      "====> Epoch: 43 Average val loss: -1.5999\n",
      "Min loss -1.60\n",
      "====> Epoch: 44 Average train loss: -1.6109\n",
      "====> Epoch: 44 Average val loss: -1.6073\n",
      "Min loss -1.61\n",
      "====> Epoch: 45 Average train loss: -1.6432\n",
      "====> Epoch: 45 Average val loss: -1.5724\n",
      "====> Epoch: 46 Average train loss: -1.6265\n",
      "====> Epoch: 46 Average val loss: -1.5814\n",
      "====> Epoch: 47 Average train loss: -1.6372\n",
      "====> Epoch: 47 Average val loss: -1.5689\n",
      "====> Epoch: 48 Average train loss: -1.6418\n",
      "====> Epoch: 48 Average val loss: -1.5550\n",
      "====> Epoch: 49 Average train loss: -1.6814\n",
      "====> Epoch: 49 Average val loss: -1.5606\n",
      "====> Epoch: 50 Average train loss: -1.6714\n",
      "====> Epoch: 50 Average val loss: -1.5689\n",
      "DMCCA\n",
      "total parameters:  201476\n",
      "====> Epoch: 1 Average train loss: -0.1487\n",
      "====> Epoch: 1 Average val loss: -0.5470\n",
      "Min loss -0.55\n",
      "====> Epoch: 2 Average train loss: -0.4530\n",
      "====> Epoch: 2 Average val loss: -1.1600\n",
      "Min loss -1.16\n",
      "====> Epoch: 3 Average train loss: -1.1947\n",
      "====> Epoch: 3 Average val loss: -1.2456\n",
      "Min loss -1.25\n",
      "====> Epoch: 4 Average train loss: -1.2807\n",
      "====> Epoch: 4 Average val loss: -1.2540\n",
      "Min loss -1.25\n",
      "====> Epoch: 5 Average train loss: -1.2983\n",
      "====> Epoch: 5 Average val loss: -1.2865\n",
      "Min loss -1.29\n",
      "====> Epoch: 6 Average train loss: -1.3456\n",
      "====> Epoch: 6 Average val loss: -1.3093\n",
      "Min loss -1.31\n",
      "====> Epoch: 7 Average train loss: -1.3680\n",
      "====> Epoch: 7 Average val loss: -1.3388\n",
      "Min loss -1.34\n",
      "====> Epoch: 8 Average train loss: -1.3474\n",
      "====> Epoch: 8 Average val loss: -1.3546\n",
      "Min loss -1.35\n",
      "====> Epoch: 9 Average train loss: -1.3714\n",
      "====> Epoch: 9 Average val loss: -1.3591\n",
      "Min loss -1.36\n",
      "====> Epoch: 10 Average train loss: -1.4154\n",
      "====> Epoch: 10 Average val loss: -1.4064\n",
      "Min loss -1.41\n",
      "====> Epoch: 11 Average train loss: -1.4595\n",
      "====> Epoch: 11 Average val loss: -1.3986\n",
      "====> Epoch: 12 Average train loss: -1.4482\n",
      "====> Epoch: 12 Average val loss: -1.4810\n",
      "Min loss -1.48\n",
      "====> Epoch: 13 Average train loss: -1.4613\n",
      "====> Epoch: 13 Average val loss: -1.4201\n",
      "====> Epoch: 14 Average train loss: -1.4873\n",
      "====> Epoch: 14 Average val loss: -1.4529\n",
      "====> Epoch: 15 Average train loss: -1.5220\n",
      "====> Epoch: 15 Average val loss: -1.4415\n",
      "====> Epoch: 16 Average train loss: -1.5480\n",
      "====> Epoch: 16 Average val loss: -1.4601\n",
      "====> Epoch: 17 Average train loss: -1.5325\n",
      "====> Epoch: 17 Average val loss: -1.4803\n",
      "====> Epoch: 18 Average train loss: -1.5061\n",
      "====> Epoch: 18 Average val loss: -1.4160\n",
      "====> Epoch: 19 Average train loss: -1.5260\n",
      "====> Epoch: 19 Average val loss: -1.5018\n",
      "Min loss -1.50\n",
      "====> Epoch: 20 Average train loss: -1.5677\n",
      "====> Epoch: 20 Average val loss: -1.4661\n",
      "====> Epoch: 21 Average train loss: -1.5572\n",
      "====> Epoch: 21 Average val loss: -1.5250\n",
      "Min loss -1.52\n",
      "====> Epoch: 22 Average train loss: -1.5382\n",
      "====> Epoch: 22 Average val loss: -1.5167\n",
      "====> Epoch: 23 Average train loss: -1.5586\n",
      "====> Epoch: 23 Average val loss: -1.5228\n",
      "====> Epoch: 24 Average train loss: -1.5783\n",
      "====> Epoch: 24 Average val loss: -1.5129\n",
      "====> Epoch: 25 Average train loss: -1.5760\n",
      "====> Epoch: 25 Average val loss: -1.5100\n",
      "====> Epoch: 26 Average train loss: -1.6222\n",
      "====> Epoch: 26 Average val loss: -1.5632\n",
      "Min loss -1.56\n",
      "====> Epoch: 27 Average train loss: -1.6132\n",
      "====> Epoch: 27 Average val loss: -1.5388\n",
      "====> Epoch: 28 Average train loss: -1.6067\n",
      "====> Epoch: 28 Average val loss: -1.5503\n",
      "====> Epoch: 29 Average train loss: -1.5983\n",
      "====> Epoch: 29 Average val loss: -1.5422\n",
      "====> Epoch: 30 Average train loss: -1.5742\n",
      "====> Epoch: 30 Average val loss: -1.5606\n",
      "====> Epoch: 31 Average train loss: -1.6166\n",
      "====> Epoch: 31 Average val loss: -1.5473\n",
      "====> Epoch: 32 Average train loss: -1.5974\n",
      "====> Epoch: 32 Average val loss: -1.5653\n",
      "Min loss -1.57\n",
      "====> Epoch: 33 Average train loss: -1.6513\n",
      "====> Epoch: 33 Average val loss: -1.5988\n",
      "Min loss -1.60\n",
      "====> Epoch: 34 Average train loss: -1.6280\n",
      "====> Epoch: 34 Average val loss: -1.5615\n",
      "====> Epoch: 35 Average train loss: -1.6346\n",
      "====> Epoch: 35 Average val loss: -1.5857\n",
      "====> Epoch: 36 Average train loss: -1.6809\n",
      "====> Epoch: 36 Average val loss: -1.5963\n",
      "====> Epoch: 37 Average train loss: -1.6387\n",
      "====> Epoch: 37 Average val loss: -1.5898\n",
      "====> Epoch: 38 Average train loss: -1.6517\n",
      "====> Epoch: 38 Average val loss: -1.5802\n",
      "====> Epoch: 39 Average train loss: -1.6758\n",
      "====> Epoch: 39 Average val loss: -1.6001\n",
      "Min loss -1.60\n",
      "====> Epoch: 40 Average train loss: -1.6735\n",
      "====> Epoch: 40 Average val loss: -1.5999\n",
      "====> Epoch: 41 Average train loss: -1.6641\n",
      "====> Epoch: 41 Average val loss: -1.5757\n",
      "====> Epoch: 42 Average train loss: -1.6705\n",
      "====> Epoch: 42 Average val loss: -1.5748\n",
      "====> Epoch: 43 Average train loss: -1.6504\n",
      "====> Epoch: 43 Average val loss: -1.5975\n",
      "====> Epoch: 44 Average train loss: -1.6884\n",
      "====> Epoch: 44 Average val loss: -1.5888\n",
      "====> Epoch: 45 Average train loss: -1.6462\n",
      "====> Epoch: 45 Average val loss: -1.5880\n",
      "====> Epoch: 46 Average train loss: -1.6763\n",
      "====> Epoch: 46 Average val loss: -1.6364\n",
      "Min loss -1.64\n",
      "====> Epoch: 47 Average train loss: -1.6731\n",
      "====> Epoch: 47 Average val loss: -1.5719\n",
      "====> Epoch: 48 Average train loss: -1.6682\n",
      "====> Epoch: 48 Average val loss: -1.6020\n",
      "====> Epoch: 49 Average train loss: -1.6739\n",
      "====> Epoch: 49 Average val loss: -1.6420\n",
      "Min loss -1.64\n",
      "====> Epoch: 50 Average train loss: -1.6892\n",
      "====> Epoch: 50 Average val loss: -1.6354\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym0WfC6uOA-8"
   },
   "source": [
    "# Deep Canonically Correlated Autoencoders\n",
    "We need to add decoders in order to model deep canonically correlated autoencoders and we also use the DCCAE class which inherits from DCCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwiNJf-MOBOg",
    "outputId": "bd8ed207-e5f0-4e03-8e34-c349b7b44d96"
   },
   "source": [
    "from cca_zoo import dccae\n",
    "\n",
    "# DCCAE\n",
    "print('DCCAE')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "decoder_1 = deep_models.Decoder(latent_dims=latent_dims, feature_size=784)\n",
    "decoder_2 = deep_models.Decoder(latent_dims=latent_dims, feature_size=784)\n",
    "dccae_model = dccae.DCCAE(latent_dims=latent_dims, encoders=[encoder_1, encoder_2], decoders=[decoder_1, decoder_2])\n",
    "\n",
    "dccae_model = deepwrapper.DeepWrapper(dccae_model)\n",
    "\n",
    "#can also pass a tuple of numpy arrays\n",
    "dccae_model.fit((train_view_1, train_view_2), epochs=epochs)\n",
    "\n",
    "dccae_results = np.stack(\n",
    "    (dccae_model.train_correlations[0, 1], dccae_model.predict_corr(test_dataset)[0, 1]))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "DCCAE\n",
      "total parameters:  404516\n",
      "====> Epoch: 1 Average train loss: 54437.5664\n",
      "====> Epoch: 1 Average val loss: 11212.0869\n",
      "Min loss 11212.09\n",
      "====> Epoch: 2 Average train loss: 45825.0508\n",
      "====> Epoch: 2 Average val loss: 9642.8906\n",
      "Min loss 9642.89\n",
      "====> Epoch: 3 Average train loss: 39497.9688\n",
      "====> Epoch: 3 Average val loss: 8547.5625\n",
      "Min loss 8547.56\n",
      "====> Epoch: 4 Average train loss: 35070.7930\n",
      "====> Epoch: 4 Average val loss: 7814.0234\n",
      "Min loss 7814.02\n",
      "====> Epoch: 5 Average train loss: 32095.4863\n",
      "====> Epoch: 5 Average val loss: 7340.1987\n",
      "Min loss 7340.20\n",
      "====> Epoch: 6 Average train loss: 30163.4785\n",
      "====> Epoch: 6 Average val loss: 7048.8726\n",
      "Min loss 7048.87\n",
      "====> Epoch: 7 Average train loss: 28966.8574\n",
      "====> Epoch: 7 Average val loss: 6882.0151\n",
      "Min loss 6882.02\n",
      "====> Epoch: 8 Average train loss: 28274.3203\n",
      "====> Epoch: 8 Average val loss: 6794.5645\n",
      "Min loss 6794.56\n",
      "====> Epoch: 9 Average train loss: 27904.0176\n",
      "====> Epoch: 9 Average val loss: 6751.9502\n",
      "Min loss 6751.95\n",
      "====> Epoch: 10 Average train loss: 27715.8105\n",
      "====> Epoch: 10 Average val loss: 6731.8882\n",
      "Min loss 6731.89\n",
      "====> Epoch: 11 Average train loss: 27618.8105\n",
      "====> Epoch: 11 Average val loss: 6721.9644\n",
      "Min loss 6721.96\n",
      "====> Epoch: 12 Average train loss: 27563.2617\n",
      "====> Epoch: 12 Average val loss: 6714.7910\n",
      "Min loss 6714.79\n",
      "====> Epoch: 13 Average train loss: 27520.6484\n",
      "====> Epoch: 13 Average val loss: 6704.5610\n",
      "Min loss 6704.56\n",
      "====> Epoch: 14 Average train loss: 27468.6719\n",
      "====> Epoch: 14 Average val loss: 6686.6714\n",
      "Min loss 6686.67\n",
      "====> Epoch: 15 Average train loss: 27389.6309\n",
      "====> Epoch: 15 Average val loss: 6659.0669\n",
      "Min loss 6659.07\n",
      "====> Epoch: 16 Average train loss: 27276.1855\n",
      "====> Epoch: 16 Average val loss: 6622.4180\n",
      "Min loss 6622.42\n",
      "====> Epoch: 17 Average train loss: 27131.6582\n",
      "====> Epoch: 17 Average val loss: 6578.6411\n",
      "Min loss 6578.64\n",
      "====> Epoch: 18 Average train loss: 26963.9922\n",
      "====> Epoch: 18 Average val loss: 6529.3301\n",
      "Min loss 6529.33\n",
      "====> Epoch: 19 Average train loss: 26779.6934\n",
      "====> Epoch: 19 Average val loss: 6475.3994\n",
      "Min loss 6475.40\n",
      "====> Epoch: 20 Average train loss: 26582.3398\n",
      "====> Epoch: 20 Average val loss: 6417.6006\n",
      "Min loss 6417.60\n",
      "====> Epoch: 21 Average train loss: 26374.3574\n",
      "====> Epoch: 21 Average val loss: 6357.0034\n",
      "Min loss 6357.00\n",
      "====> Epoch: 22 Average train loss: 26158.6836\n",
      "====> Epoch: 22 Average val loss: 6294.8608\n",
      "Min loss 6294.86\n",
      "====> Epoch: 23 Average train loss: 25938.7363\n",
      "====> Epoch: 23 Average val loss: 6232.4194\n",
      "Min loss 6232.42\n",
      "====> Epoch: 24 Average train loss: 25717.5859\n",
      "====> Epoch: 24 Average val loss: 6170.6523\n",
      "Min loss 6170.65\n",
      "====> Epoch: 25 Average train loss: 25497.4199\n",
      "====> Epoch: 25 Average val loss: 6110.1533\n",
      "Min loss 6110.15\n",
      "====> Epoch: 26 Average train loss: 25279.3945\n",
      "====> Epoch: 26 Average val loss: 6051.1533\n",
      "Min loss 6051.15\n",
      "====> Epoch: 27 Average train loss: 25063.9004\n",
      "====> Epoch: 27 Average val loss: 5993.6816\n",
      "Min loss 5993.68\n",
      "====> Epoch: 28 Average train loss: 24851.0273\n",
      "====> Epoch: 28 Average val loss: 5937.6890\n",
      "Min loss 5937.69\n",
      "====> Epoch: 29 Average train loss: 24640.9062\n",
      "====> Epoch: 29 Average val loss: 5883.1353\n",
      "Min loss 5883.14\n",
      "====> Epoch: 30 Average train loss: 24433.7207\n",
      "====> Epoch: 30 Average val loss: 5829.9751\n",
      "Min loss 5829.98\n",
      "====> Epoch: 31 Average train loss: 24229.5879\n",
      "====> Epoch: 31 Average val loss: 5778.1904\n",
      "Min loss 5778.19\n",
      "====> Epoch: 32 Average train loss: 24028.5898\n",
      "====> Epoch: 32 Average val loss: 5727.7925\n",
      "Min loss 5727.79\n",
      "====> Epoch: 33 Average train loss: 23830.8242\n",
      "====> Epoch: 33 Average val loss: 5678.7603\n",
      "Min loss 5678.76\n",
      "====> Epoch: 34 Average train loss: 23636.2480\n",
      "====> Epoch: 34 Average val loss: 5630.9619\n",
      "Min loss 5630.96\n",
      "====> Epoch: 35 Average train loss: 23444.5273\n",
      "====> Epoch: 35 Average val loss: 5584.2104\n",
      "Min loss 5584.21\n",
      "====> Epoch: 36 Average train loss: 23255.3613\n",
      "====> Epoch: 36 Average val loss: 5538.3042\n",
      "Min loss 5538.30\n",
      "====> Epoch: 37 Average train loss: 23068.5957\n",
      "====> Epoch: 37 Average val loss: 5493.0386\n",
      "Min loss 5493.04\n",
      "====> Epoch: 38 Average train loss: 22884.0938\n",
      "====> Epoch: 38 Average val loss: 5448.3027\n",
      "Min loss 5448.30\n",
      "====> Epoch: 39 Average train loss: 22701.8652\n",
      "====> Epoch: 39 Average val loss: 5404.1597\n",
      "Min loss 5404.16\n",
      "====> Epoch: 40 Average train loss: 22522.2520\n",
      "====> Epoch: 40 Average val loss: 5360.7319\n",
      "Min loss 5360.73\n",
      "====> Epoch: 41 Average train loss: 22345.5391\n",
      "====> Epoch: 41 Average val loss: 5318.1562\n",
      "Min loss 5318.16\n",
      "====> Epoch: 42 Average train loss: 22171.9922\n",
      "====> Epoch: 42 Average val loss: 5276.6196\n",
      "Min loss 5276.62\n",
      "====> Epoch: 43 Average train loss: 22002.1504\n",
      "====> Epoch: 43 Average val loss: 5236.3237\n",
      "Min loss 5236.32\n",
      "====> Epoch: 44 Average train loss: 21836.6211\n",
      "====> Epoch: 44 Average val loss: 5197.4575\n",
      "Min loss 5197.46\n",
      "====> Epoch: 45 Average train loss: 21675.8887\n",
      "====> Epoch: 45 Average val loss: 5160.2217\n",
      "Min loss 5160.22\n",
      "====> Epoch: 46 Average train loss: 21520.5312\n",
      "====> Epoch: 46 Average val loss: 5124.7563\n",
      "Min loss 5124.76\n",
      "====> Epoch: 47 Average train loss: 21371.0840\n",
      "====> Epoch: 47 Average val loss: 5091.0669\n",
      "Min loss 5091.07\n",
      "====> Epoch: 48 Average train loss: 21227.7480\n",
      "====> Epoch: 48 Average val loss: 5059.1050\n",
      "Min loss 5059.10\n",
      "====> Epoch: 49 Average train loss: 21090.6113\n",
      "====> Epoch: 49 Average val loss: 5028.8188\n",
      "Min loss 5028.82\n",
      "====> Epoch: 50 Average train loss: 20959.7188\n",
      "====> Epoch: 50 Average val loss: 5000.1206\n",
      "Min loss 5000.12\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwOTBYZ0O9SP"
   },
   "source": [
    "# Deep Variational CCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SogqelCuO9rm",
    "outputId": "c04a1d86-6204-4164-d78a-8958b037e836"
   },
   "source": [
    "\"\"\"\n",
    "### Deep Variational Learning\n",
    "Finally we have Deep Variational CCA methods.\n",
    "- Deep Variational CCA (DVCCA)\n",
    "- Deep Variational CCA - private (DVVCA_p)\n",
    "\n",
    "These are both implemented by the DVCCA class with private=True/False and both_encoders=True/False. If both_encoders,\n",
    "the encoder to the shared information Q(z_shared|x) is modelled for both x_1 and x_2 whereas if both_encoders is false\n",
    "it is modelled for x_1 as in the paper\n",
    "\"\"\"\n",
    "from cca_zoo import dvcca\n",
    "\n",
    "# %%\n",
    "# DVCCA (technically bi-DVCCA)\n",
    "print('DVCCA')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784, variational=True)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784, variational=True)\n",
    "decoder_1 = deep_models.Decoder(latent_dims=latent_dims, feature_size=784, norm_output=True)\n",
    "decoder_2 = deep_models.Decoder(latent_dims=latent_dims, feature_size=784, norm_output=True)\n",
    "dvcca_model = dvcca.DVCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2], decoders=[decoder_1, decoder_2],\n",
    "                          private=False)\n",
    "\n",
    "dvcca_model = deepwrapper.DeepWrapper(dvcca_model)\n",
    "\n",
    "dvcca_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dvcca_model_results = np.stack(\n",
    "    (dvcca_model.train_correlations[0, 1], dvcca_model.predict_corr(test_dataset)[0, 1]))\n",
    "\n",
    "# DVCCA_private (technically bi-DVCCA_private)\n",
    "print('DVCCA_private')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784, variational=True)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784, variational=True)\n",
    "private_encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784, variational=True)\n",
    "private_encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784, variational=True)\n",
    "decoder_1 = deep_models.Decoder(latent_dims=latent_dims * 2, feature_size=784, norm_output=True)\n",
    "decoder_2 = deep_models.Decoder(latent_dims=latent_dims * 2, feature_size=784, norm_output=True)\n",
    "dvccap_model = dvcca.DVCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2], decoders=[decoder_1, decoder_2],\n",
    "                           private_encoders=[private_encoder_1, private_encoder_2], private=True)\n",
    "\n",
    "dvccap_model = deepwrapper.DeepWrapper(dvccap_model)\n",
    "\n",
    "dvccap_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dvccap_model_results = np.stack(\n",
    "    (dvccap_model.train_correlations[0, 1], dvccap_model.predict_corr(test_dataset)[0, 1]))"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "DVCCA\n",
      "total parameters:  405032\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average train loss: 1102.2251\n",
      "====> Epoch: 1 Average val loss: 1080.9238\n",
      "Min loss 1080.92\n",
      "====> Epoch: 2 Average train loss: 1080.6416\n",
      "====> Epoch: 2 Average val loss: 1061.2964\n",
      "Min loss 1061.30\n",
      "====> Epoch: 3 Average train loss: 1060.2611\n",
      "====> Epoch: 3 Average val loss: 1042.7833\n",
      "Min loss 1042.78\n",
      "====> Epoch: 4 Average train loss: 1042.7712\n",
      "====> Epoch: 4 Average val loss: 1025.0345\n",
      "Min loss 1025.03\n",
      "====> Epoch: 5 Average train loss: 1026.2625\n",
      "====> Epoch: 5 Average val loss: 1011.1165\n",
      "Min loss 1011.12\n",
      "====> Epoch: 6 Average train loss: 1009.3911\n",
      "====> Epoch: 6 Average val loss: 995.5959\n",
      "Min loss 995.60\n",
      "====> Epoch: 7 Average train loss: 995.2148\n",
      "====> Epoch: 7 Average val loss: 983.6167\n",
      "Min loss 983.62\n",
      "====> Epoch: 8 Average train loss: 981.9344\n",
      "====> Epoch: 8 Average val loss: 971.5005\n",
      "Min loss 971.50\n",
      "====> Epoch: 9 Average train loss: 968.9379\n",
      "====> Epoch: 9 Average val loss: 960.5873\n",
      "Min loss 960.59\n",
      "====> Epoch: 10 Average train loss: 959.9377\n",
      "====> Epoch: 10 Average val loss: 949.4959\n",
      "Min loss 949.50\n",
      "====> Epoch: 11 Average train loss: 950.4307\n",
      "====> Epoch: 11 Average val loss: 941.1023\n",
      "Min loss 941.10\n",
      "====> Epoch: 12 Average train loss: 939.9617\n",
      "====> Epoch: 12 Average val loss: 934.3436\n",
      "Min loss 934.34\n",
      "====> Epoch: 13 Average train loss: 932.3995\n",
      "====> Epoch: 13 Average val loss: 925.1294\n",
      "Min loss 925.13\n",
      "====> Epoch: 14 Average train loss: 924.1914\n",
      "====> Epoch: 14 Average val loss: 918.9165\n",
      "Min loss 918.92\n",
      "====> Epoch: 15 Average train loss: 918.0424\n",
      "====> Epoch: 15 Average val loss: 915.2806\n",
      "Min loss 915.28\n",
      "====> Epoch: 16 Average train loss: 914.9824\n",
      "====> Epoch: 16 Average val loss: 909.9840\n",
      "Min loss 909.98\n",
      "====> Epoch: 17 Average train loss: 908.9607\n",
      "====> Epoch: 17 Average val loss: 906.6709\n",
      "Min loss 906.67\n",
      "====> Epoch: 18 Average train loss: 901.2586\n",
      "====> Epoch: 18 Average val loss: 899.9290\n",
      "Min loss 899.93\n",
      "====> Epoch: 19 Average train loss: 896.7518\n",
      "====> Epoch: 19 Average val loss: 897.5199\n",
      "Min loss 897.52\n",
      "====> Epoch: 20 Average train loss: 890.4279\n",
      "====> Epoch: 20 Average val loss: 889.4216\n",
      "Min loss 889.42\n",
      "====> Epoch: 21 Average train loss: 890.5521\n",
      "====> Epoch: 21 Average val loss: 887.8289\n",
      "Min loss 887.83\n",
      "====> Epoch: 22 Average train loss: 888.1741\n",
      "====> Epoch: 22 Average val loss: 888.0970\n",
      "====> Epoch: 23 Average train loss: 881.5704\n",
      "====> Epoch: 23 Average val loss: 884.2130\n",
      "Min loss 884.21\n",
      "====> Epoch: 24 Average train loss: 879.3253\n",
      "====> Epoch: 24 Average val loss: 879.8125\n",
      "Min loss 879.81\n",
      "====> Epoch: 25 Average train loss: 879.7351\n",
      "====> Epoch: 25 Average val loss: 879.8800\n",
      "====> Epoch: 26 Average train loss: 876.0739\n",
      "====> Epoch: 26 Average val loss: 876.0736\n",
      "Min loss 876.07\n",
      "====> Epoch: 27 Average train loss: 871.3176\n",
      "====> Epoch: 27 Average val loss: 873.9788\n",
      "Min loss 873.98\n",
      "====> Epoch: 28 Average train loss: 869.8440\n",
      "====> Epoch: 28 Average val loss: 872.1866\n",
      "Min loss 872.19\n",
      "====> Epoch: 29 Average train loss: 872.3887\n",
      "====> Epoch: 29 Average val loss: 872.0831\n",
      "Min loss 872.08\n",
      "====> Epoch: 30 Average train loss: 868.8909\n",
      "====> Epoch: 30 Average val loss: 873.9348\n",
      "====> Epoch: 31 Average train loss: 865.4429\n",
      "====> Epoch: 31 Average val loss: 867.3519\n",
      "Min loss 867.35\n",
      "====> Epoch: 32 Average train loss: 864.2915\n",
      "====> Epoch: 32 Average val loss: 864.5686\n",
      "Min loss 864.57\n",
      "====> Epoch: 33 Average train loss: 866.6432\n",
      "====> Epoch: 33 Average val loss: 865.1965\n",
      "====> Epoch: 34 Average train loss: 866.7880\n",
      "====> Epoch: 34 Average val loss: 861.1844\n",
      "Min loss 861.18\n",
      "====> Epoch: 35 Average train loss: 860.3547\n",
      "====> Epoch: 35 Average val loss: 867.1351\n",
      "====> Epoch: 36 Average train loss: 862.2640\n",
      "====> Epoch: 36 Average val loss: 861.6696\n",
      "====> Epoch: 37 Average train loss: 858.3754\n",
      "====> Epoch: 37 Average val loss: 860.6166\n",
      "Min loss 860.62\n",
      "====> Epoch: 38 Average train loss: 858.6259\n",
      "====> Epoch: 38 Average val loss: 857.6047\n",
      "Min loss 857.60\n",
      "====> Epoch: 39 Average train loss: 860.0117\n",
      "====> Epoch: 39 Average val loss: 857.3486\n",
      "Min loss 857.35\n",
      "====> Epoch: 40 Average train loss: 856.9528\n",
      "====> Epoch: 40 Average val loss: 857.7258\n",
      "====> Epoch: 41 Average train loss: 857.3808\n",
      "====> Epoch: 41 Average val loss: 858.1618\n",
      "====> Epoch: 42 Average train loss: 853.6931\n",
      "====> Epoch: 42 Average val loss: 854.9758\n",
      "Min loss 854.98\n",
      "====> Epoch: 43 Average train loss: 853.3806\n",
      "====> Epoch: 43 Average val loss: 857.9451\n",
      "====> Epoch: 44 Average train loss: 857.5377\n",
      "====> Epoch: 44 Average val loss: 860.1490\n",
      "====> Epoch: 45 Average train loss: 854.0601\n",
      "====> Epoch: 45 Average val loss: 852.6035\n",
      "Min loss 852.60\n",
      "====> Epoch: 46 Average train loss: 851.3213\n",
      "====> Epoch: 46 Average val loss: 853.7871\n",
      "====> Epoch: 47 Average train loss: 850.3039\n",
      "====> Epoch: 47 Average val loss: 852.4201\n",
      "Min loss 852.42\n",
      "====> Epoch: 48 Average train loss: 849.4520\n",
      "====> Epoch: 48 Average val loss: 850.8690\n",
      "Min loss 850.87\n",
      "====> Epoch: 49 Average train loss: 848.2947\n",
      "====> Epoch: 49 Average val loss: 847.4043\n",
      "Min loss 847.40\n",
      "====> Epoch: 50 Average train loss: 849.7018\n",
      "====> Epoch: 50 Average val loss: 851.6854\n",
      "DVCCA_private\n",
      "total parameters:  607536\n",
      "====> Epoch: 1 Average train loss: 1102.9314\n",
      "====> Epoch: 1 Average val loss: 1081.7896\n",
      "Min loss 1081.79\n",
      "====> Epoch: 2 Average train loss: 1081.4987\n",
      "====> Epoch: 2 Average val loss: 1061.6177\n",
      "Min loss 1061.62\n",
      "====> Epoch: 3 Average train loss: 1061.5649\n",
      "====> Epoch: 3 Average val loss: 1043.1111\n",
      "Min loss 1043.11\n",
      "====> Epoch: 4 Average train loss: 1043.5857\n",
      "====> Epoch: 4 Average val loss: 1026.6394\n",
      "Min loss 1026.64\n",
      "====> Epoch: 5 Average train loss: 1026.1855\n",
      "====> Epoch: 5 Average val loss: 1010.3207\n",
      "Min loss 1010.32\n",
      "====> Epoch: 6 Average train loss: 1010.6351\n",
      "====> Epoch: 6 Average val loss: 996.4670\n",
      "Min loss 996.47\n",
      "====> Epoch: 7 Average train loss: 997.2227\n",
      "====> Epoch: 7 Average val loss: 983.2792\n",
      "Min loss 983.28\n",
      "====> Epoch: 8 Average train loss: 983.5792\n",
      "====> Epoch: 8 Average val loss: 972.3938\n",
      "Min loss 972.39\n",
      "====> Epoch: 9 Average train loss: 971.1459\n",
      "====> Epoch: 9 Average val loss: 961.0009\n",
      "Min loss 961.00\n",
      "====> Epoch: 10 Average train loss: 960.3127\n",
      "====> Epoch: 10 Average val loss: 951.2385\n",
      "Min loss 951.24\n",
      "====> Epoch: 11 Average train loss: 949.7381\n",
      "====> Epoch: 11 Average val loss: 941.8234\n",
      "Min loss 941.82\n",
      "====> Epoch: 12 Average train loss: 942.4531\n",
      "====> Epoch: 12 Average val loss: 934.0258\n",
      "Min loss 934.03\n",
      "====> Epoch: 13 Average train loss: 933.6135\n",
      "====> Epoch: 13 Average val loss: 926.2821\n",
      "Min loss 926.28\n",
      "====> Epoch: 14 Average train loss: 925.0591\n",
      "====> Epoch: 14 Average val loss: 919.9098\n",
      "Min loss 919.91\n",
      "====> Epoch: 15 Average train loss: 919.1109\n",
      "====> Epoch: 15 Average val loss: 912.8807\n",
      "Min loss 912.88\n",
      "====> Epoch: 16 Average train loss: 912.0072\n",
      "====> Epoch: 16 Average val loss: 908.9057\n",
      "Min loss 908.91\n",
      "====> Epoch: 17 Average train loss: 907.6754\n",
      "====> Epoch: 17 Average val loss: 903.9383\n",
      "Min loss 903.94\n",
      "====> Epoch: 18 Average train loss: 903.5721\n",
      "====> Epoch: 18 Average val loss: 901.1700\n",
      "Min loss 901.17\n",
      "====> Epoch: 19 Average train loss: 897.3947\n",
      "====> Epoch: 19 Average val loss: 892.9937\n",
      "Min loss 892.99\n",
      "====> Epoch: 20 Average train loss: 893.8175\n",
      "====> Epoch: 20 Average val loss: 889.4621\n",
      "Min loss 889.46\n",
      "====> Epoch: 21 Average train loss: 891.7983\n",
      "====> Epoch: 21 Average val loss: 886.9846\n",
      "Min loss 886.98\n",
      "====> Epoch: 22 Average train loss: 884.9601\n",
      "====> Epoch: 22 Average val loss: 884.9901\n",
      "Min loss 884.99\n",
      "====> Epoch: 23 Average train loss: 882.9547\n",
      "====> Epoch: 23 Average val loss: 879.2174\n",
      "Min loss 879.22\n",
      "====> Epoch: 24 Average train loss: 880.9197\n",
      "====> Epoch: 24 Average val loss: 882.3079\n",
      "====> Epoch: 25 Average train loss: 878.9204\n",
      "====> Epoch: 25 Average val loss: 880.6443\n",
      "====> Epoch: 26 Average train loss: 874.2271\n",
      "====> Epoch: 26 Average val loss: 876.8527\n",
      "Min loss 876.85\n",
      "====> Epoch: 27 Average train loss: 874.7329\n",
      "====> Epoch: 27 Average val loss: 874.2915\n",
      "Min loss 874.29\n",
      "====> Epoch: 28 Average train loss: 871.8181\n",
      "====> Epoch: 28 Average val loss: 871.0239\n",
      "Min loss 871.02\n",
      "====> Epoch: 29 Average train loss: 869.5598\n",
      "====> Epoch: 29 Average val loss: 872.2976\n",
      "====> Epoch: 30 Average train loss: 871.2627\n",
      "====> Epoch: 30 Average val loss: 868.0957\n",
      "Min loss 868.10\n",
      "====> Epoch: 31 Average train loss: 869.6201\n",
      "====> Epoch: 31 Average val loss: 869.6375\n",
      "====> Epoch: 32 Average train loss: 870.1491\n",
      "====> Epoch: 32 Average val loss: 864.6862\n",
      "Min loss 864.69\n",
      "====> Epoch: 33 Average train loss: 860.3743\n",
      "====> Epoch: 33 Average val loss: 863.9731\n",
      "Min loss 863.97\n",
      "====> Epoch: 34 Average train loss: 864.0156\n",
      "====> Epoch: 34 Average val loss: 861.9691\n",
      "Min loss 861.97\n",
      "====> Epoch: 35 Average train loss: 863.6475\n",
      "====> Epoch: 35 Average val loss: 865.2968\n",
      "====> Epoch: 36 Average train loss: 858.5311\n",
      "====> Epoch: 36 Average val loss: 863.8771\n",
      "====> Epoch: 37 Average train loss: 864.8438\n",
      "====> Epoch: 37 Average val loss: 858.8027\n",
      "Min loss 858.80\n",
      "====> Epoch: 38 Average train loss: 862.3140\n",
      "====> Epoch: 38 Average val loss: 857.0880\n",
      "Min loss 857.09\n",
      "====> Epoch: 39 Average train loss: 858.3627\n",
      "====> Epoch: 39 Average val loss: 855.9865\n",
      "Min loss 855.99\n",
      "====> Epoch: 40 Average train loss: 857.3917\n",
      "====> Epoch: 40 Average val loss: 860.2882\n",
      "====> Epoch: 41 Average train loss: 854.8077\n",
      "====> Epoch: 41 Average val loss: 857.0485\n",
      "====> Epoch: 42 Average train loss: 855.6644\n",
      "====> Epoch: 42 Average val loss: 856.9963\n",
      "====> Epoch: 43 Average train loss: 856.8729\n",
      "====> Epoch: 43 Average val loss: 852.7888\n",
      "Min loss 852.79\n",
      "====> Epoch: 44 Average train loss: 851.2393\n",
      "====> Epoch: 44 Average val loss: 853.6592\n",
      "====> Epoch: 45 Average train loss: 852.9232\n",
      "====> Epoch: 45 Average val loss: 851.4773\n",
      "Min loss 851.48\n",
      "====> Epoch: 46 Average train loss: 851.6396\n",
      "====> Epoch: 46 Average val loss: 852.5685\n",
      "====> Epoch: 47 Average train loss: 852.6101\n",
      "====> Epoch: 47 Average val loss: 850.5543\n",
      "Min loss 850.55\n",
      "====> Epoch: 48 Average train loss: 849.8184\n",
      "====> Epoch: 48 Average val loss: 849.6871\n",
      "Min loss 849.69\n",
      "====> Epoch: 49 Average train loss: 850.4979\n",
      "====> Epoch: 49 Average val loss: 849.6903\n",
      "====> Epoch: 50 Average train loss: 845.9988\n",
      "====> Epoch: 50 Average val loss: 847.2688\n",
      "Min loss 847.27\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCMeGlxJkp93"
   },
   "source": [
    "# Convolutional Deep CCA (and using other architectures)\n",
    "We provide a standard CNN encoder and decoder but users can build their own encoders and decoders by inheriting BaseEncoder and BaseDecoder for seamless integration with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9TEto00kpn2",
    "outputId": "c4824182-783e-4e14-bcdf-66afc6187753"
   },
   "source": [
    "print('Convolutional DCCA')\n",
    "encoder_1 = deep_models.CNNEncoder(latent_dims=latent_dims, channels=[3, 3])\n",
    "encoder_2 = deep_models.CNNEncoder(latent_dims=latent_dims, channels=[3, 3])\n",
    "dcca_conv_model = dcca.DCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2])\n",
    "\n",
    "dcca_conv_model = deepwrapper.DeepWrapper(dcca_conv_model)\n",
    "\n",
    "# to change the models used change the cfg.encoder_models. We implement a CNN_Encoder and CNN_decoder as well\n",
    "# as some based on brainnet architecture in cca_zoo.deep_models. Equally you could pass your own encoder/decoder models\n",
    "\n",
    "dcca_conv_model.fit((train_view_1.reshape((-1, 1, 28, 28)), train_view_2.reshape((-1, 1, 28, 28))), epochs=epochs)\n",
    "\n",
    "dcca_conv_results = np.stack(\n",
    "    (dcca_conv_model.train_correlations[0, 1], dcca_conv_model.predict_corr((test_view_1.reshape((-1, 1, 28, 28)),\n",
    "                                                                            test_view_2.reshape(\n",
    "                                                                                (-1, 1, 28, 28))))[0, 1]))"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Convolutional DCCA\n",
      "total parameters:  9568\n",
      "====> Epoch: 1 Average train loss: -0.3282\n",
      "====> Epoch: 1 Average val loss: -0.6592\n",
      "Min loss -0.66\n",
      "====> Epoch: 2 Average train loss: -0.9536\n",
      "====> Epoch: 2 Average val loss: -0.9541\n",
      "Min loss -0.95\n",
      "====> Epoch: 3 Average train loss: -1.2399\n",
      "====> Epoch: 3 Average val loss: -1.0962\n",
      "Min loss -1.10\n",
      "====> Epoch: 4 Average train loss: -1.3620\n",
      "====> Epoch: 4 Average val loss: -1.1773\n",
      "Min loss -1.18\n",
      "====> Epoch: 5 Average train loss: -1.4348\n",
      "====> Epoch: 5 Average val loss: -1.2338\n",
      "Min loss -1.23\n",
      "====> Epoch: 6 Average train loss: -1.4889\n",
      "====> Epoch: 6 Average val loss: -1.2784\n",
      "Min loss -1.28\n",
      "====> Epoch: 7 Average train loss: -1.5335\n",
      "====> Epoch: 7 Average val loss: -1.3157\n",
      "Min loss -1.32\n",
      "====> Epoch: 8 Average train loss: -1.5716\n",
      "====> Epoch: 8 Average val loss: -1.3480\n",
      "Min loss -1.35\n",
      "====> Epoch: 9 Average train loss: -1.6045\n",
      "====> Epoch: 9 Average val loss: -1.3759\n",
      "Min loss -1.38\n",
      "====> Epoch: 10 Average train loss: -1.6324\n",
      "====> Epoch: 10 Average val loss: -1.3999\n",
      "Min loss -1.40\n",
      "====> Epoch: 11 Average train loss: -1.6558\n",
      "====> Epoch: 11 Average val loss: -1.4205\n",
      "Min loss -1.42\n",
      "====> Epoch: 12 Average train loss: -1.6754\n",
      "====> Epoch: 12 Average val loss: -1.4384\n",
      "Min loss -1.44\n",
      "====> Epoch: 13 Average train loss: -1.6924\n",
      "====> Epoch: 13 Average val loss: -1.4539\n",
      "Min loss -1.45\n",
      "====> Epoch: 14 Average train loss: -1.7080\n",
      "====> Epoch: 14 Average val loss: -1.4678\n",
      "Min loss -1.47\n",
      "====> Epoch: 15 Average train loss: -1.7231\n",
      "====> Epoch: 15 Average val loss: -1.4800\n",
      "Min loss -1.48\n",
      "====> Epoch: 16 Average train loss: -1.7380\n",
      "====> Epoch: 16 Average val loss: -1.4905\n",
      "Min loss -1.49\n",
      "====> Epoch: 17 Average train loss: -1.7525\n",
      "====> Epoch: 17 Average val loss: -1.4993\n",
      "Min loss -1.50\n",
      "====> Epoch: 18 Average train loss: -1.7666\n",
      "====> Epoch: 18 Average val loss: -1.5063\n",
      "Min loss -1.51\n",
      "====> Epoch: 19 Average train loss: -1.7799\n",
      "====> Epoch: 19 Average val loss: -1.5114\n",
      "Min loss -1.51\n",
      "====> Epoch: 20 Average train loss: -1.7925\n",
      "====> Epoch: 20 Average val loss: -1.5151\n",
      "Min loss -1.52\n",
      "====> Epoch: 21 Average train loss: -1.8042\n",
      "====> Epoch: 21 Average val loss: -1.5175\n",
      "Min loss -1.52\n",
      "====> Epoch: 22 Average train loss: -1.8150\n",
      "====> Epoch: 22 Average val loss: -1.5187\n",
      "Min loss -1.52\n",
      "====> Epoch: 23 Average train loss: -1.8250\n",
      "====> Epoch: 23 Average val loss: -1.5188\n",
      "Min loss -1.52\n",
      "====> Epoch: 24 Average train loss: -1.8344\n",
      "====> Epoch: 24 Average val loss: -1.5179\n",
      "====> Epoch: 25 Average train loss: -1.8432\n",
      "====> Epoch: 25 Average val loss: -1.5164\n",
      "====> Epoch: 26 Average train loss: -1.8515\n",
      "====> Epoch: 26 Average val loss: -1.5145\n",
      "====> Epoch: 27 Average train loss: -1.8594\n",
      "====> Epoch: 27 Average val loss: -1.5121\n",
      "====> Epoch: 28 Average train loss: -1.8668\n",
      "====> Epoch: 28 Average val loss: -1.5095\n",
      "====> Epoch: 29 Average train loss: -1.8736\n",
      "====> Epoch: 29 Average val loss: -1.5065\n",
      "====> Epoch: 30 Average train loss: -1.8797\n",
      "====> Epoch: 30 Average val loss: -1.5034\n",
      "====> Epoch: 31 Average train loss: -1.8849\n",
      "====> Epoch: 31 Average val loss: -1.5005\n",
      "====> Epoch: 32 Average train loss: -1.8896\n",
      "====> Epoch: 32 Average val loss: -1.4981\n",
      "====> Epoch: 33 Average train loss: -1.8941\n",
      "====> Epoch: 33 Average val loss: -1.4966\n",
      "====> Epoch: 34 Average train loss: -1.8988\n",
      "====> Epoch: 34 Average val loss: -1.4960\n",
      "====> Epoch: 35 Average train loss: -1.9037\n",
      "====> Epoch: 35 Average val loss: -1.4961\n",
      "====> Epoch: 36 Average train loss: -1.9087\n",
      "====> Epoch: 36 Average val loss: -1.4967\n",
      "====> Epoch: 37 Average train loss: -1.9133\n",
      "====> Epoch: 37 Average val loss: -1.4977\n",
      "====> Epoch: 38 Average train loss: -1.9174\n",
      "====> Epoch: 38 Average val loss: -1.4988\n",
      "====> Epoch: 39 Average train loss: -1.9211\n",
      "====> Epoch: 39 Average val loss: -1.5001\n",
      "====> Epoch: 40 Average train loss: -1.9245\n",
      "====> Epoch: 40 Average val loss: -1.5016\n",
      "====> Epoch: 41 Average train loss: -1.9277\n",
      "====> Epoch: 41 Average val loss: -1.5031\n",
      "====> Epoch: 42 Average train loss: -1.9308\n",
      "====> Epoch: 42 Average val loss: -1.5045\n",
      "====> Epoch: 43 Average train loss: -1.9337\n",
      "====> Epoch: 43 Average val loss: -1.5055\n",
      "====> Epoch: 44 Average train loss: -1.9363\n",
      "====> Epoch: 44 Average val loss: -1.5061\n",
      "====> Epoch: 45 Average train loss: -1.9386\n",
      "====> Epoch: 45 Average val loss: -1.5063\n",
      "====> Epoch: 46 Average train loss: -1.9407\n",
      "====> Epoch: 46 Average val loss: -1.5060\n",
      "====> Epoch: 47 Average train loss: -1.9426\n",
      "====> Epoch: 47 Average val loss: -1.5055\n",
      "====> Epoch: 48 Average train loss: -1.9446\n",
      "====> Epoch: 48 Average val loss: -1.5047\n",
      "====> Epoch: 49 Average train loss: -1.9466\n",
      "====> Epoch: 49 Average val loss: -1.5036\n",
      "====> Epoch: 50 Average train loss: -1.9485\n",
      "====> Epoch: 50 Average val loss: -1.5023\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHomuS25rNJb"
   },
   "source": [
    "# DTCCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vnINRlk7rPf0",
    "outputId": "0c3acdcd-9023-45b1-c7a6-66a645ba3440",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "from cca_zoo import dtcca\n",
    "\n",
    "# %%\n",
    "# DTCCA\n",
    "print('DTCCA')\n",
    "encoder_1 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "encoder_2 = deep_models.Encoder(latent_dims=latent_dims, feature_size=784)\n",
    "dtcca_model = dtcca.DTCCA(latent_dims=latent_dims, encoders=[encoder_1, encoder_2])\n",
    "\n",
    "dtcca_model = deepwrapper.DeepWrapper(dtcca_model)\n",
    "\n",
    "dtcca_model.fit(train_dataset, val_dataset=val_dataset, epochs=epochs)\n",
    "\n",
    "dtcca_results = np.stack((dtcca_model.train_correlations[0, 1], dtcca_model.predict_corr(test_dataset)[0, 1]))"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "DTCCA\n",
      "total parameters:  201476\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n",
      "/usr/local/lib/python3.7/dist-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.\n",
      "  warnings.warn('In partial_svd: converting to NumPy.'\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average train loss: 0.0000\n",
      "====> Epoch: 1 Average val loss: 0.0000\n",
      "Min loss 0.00\n",
      "====> Epoch: 2 Average train loss: 0.0000\n",
      "====> Epoch: 2 Average val loss: 0.0000\n",
      "Min loss 0.00\n",
      "====> Epoch: 3 Average train loss: 0.0000\n",
      "====> Epoch: 3 Average val loss: 0.0000\n",
      "====> Epoch: 4 Average train loss: 0.0000\n",
      "====> Epoch: 4 Average val loss: 0.0000\n",
      "====> Epoch: 5 Average train loss: 0.0000\n",
      "====> Epoch: 5 Average val loss: 0.0000\n",
      "====> Epoch: 6 Average train loss: 0.0000\n",
      "====> Epoch: 6 Average val loss: 0.0000\n",
      "====> Epoch: 7 Average train loss: 0.0000\n",
      "====> Epoch: 7 Average val loss: 0.0000\n",
      "====> Epoch: 8 Average train loss: 0.0000\n",
      "====> Epoch: 8 Average val loss: 0.0000\n",
      "====> Epoch: 9 Average train loss: 0.0000\n",
      "====> Epoch: 9 Average val loss: 0.0000\n",
      "====> Epoch: 10 Average train loss: 0.0000\n",
      "====> Epoch: 10 Average val loss: 0.0000\n",
      "====> Epoch: 11 Average train loss: 0.0000\n",
      "====> Epoch: 11 Average val loss: 0.0000\n",
      "====> Epoch: 12 Average train loss: 0.0000\n",
      "====> Epoch: 12 Average val loss: 0.0000\n",
      "====> Epoch: 13 Average train loss: 0.0000\n",
      "====> Epoch: 13 Average val loss: 0.0000\n",
      "====> Epoch: 14 Average train loss: 0.0000\n",
      "====> Epoch: 14 Average val loss: 0.0000\n",
      "====> Epoch: 15 Average train loss: 0.0000\n",
      "====> Epoch: 15 Average val loss: 0.0000\n",
      "====> Epoch: 16 Average train loss: 0.0000\n",
      "====> Epoch: 16 Average val loss: 0.0000\n",
      "====> Epoch: 17 Average train loss: 0.0000\n",
      "====> Epoch: 17 Average val loss: 0.0000\n",
      "====> Epoch: 18 Average train loss: 0.0000\n",
      "====> Epoch: 18 Average val loss: 0.0000\n",
      "====> Epoch: 19 Average train loss: 0.0000\n",
      "====> Epoch: 19 Average val loss: 0.0000\n",
      "====> Epoch: 20 Average train loss: 0.0000\n",
      "====> Epoch: 20 Average val loss: 0.0000\n",
      "====> Epoch: 21 Average train loss: 0.0000\n",
      "====> Epoch: 21 Average val loss: 0.0000\n",
      "====> Epoch: 22 Average train loss: 0.0000\n",
      "====> Epoch: 22 Average val loss: 0.0000\n",
      "====> Epoch: 23 Average train loss: 0.0000\n",
      "====> Epoch: 23 Average val loss: 0.0000\n",
      "====> Epoch: 24 Average train loss: 0.0000\n",
      "====> Epoch: 24 Average val loss: 0.0000\n",
      "====> Epoch: 25 Average train loss: 0.0000\n",
      "====> Epoch: 25 Average val loss: 0.0000\n",
      "====> Epoch: 26 Average train loss: 0.0000\n",
      "====> Epoch: 26 Average val loss: 0.0000\n",
      "====> Epoch: 27 Average train loss: 0.0000\n",
      "====> Epoch: 27 Average val loss: 0.0000\n",
      "====> Epoch: 28 Average train loss: 0.0000\n",
      "====> Epoch: 28 Average val loss: 0.0000\n",
      "====> Epoch: 29 Average train loss: 0.0000\n",
      "====> Epoch: 29 Average val loss: 0.0000\n",
      "====> Epoch: 30 Average train loss: 0.0000\n",
      "====> Epoch: 30 Average val loss: 0.0000\n",
      "====> Epoch: 31 Average train loss: 0.0000\n",
      "====> Epoch: 31 Average val loss: 0.0000\n",
      "====> Epoch: 32 Average train loss: 0.0000\n",
      "====> Epoch: 32 Average val loss: 0.0000\n",
      "====> Epoch: 33 Average train loss: 0.0000\n",
      "====> Epoch: 33 Average val loss: 0.0000\n",
      "====> Epoch: 34 Average train loss: 0.0000\n",
      "====> Epoch: 34 Average val loss: 0.0000\n",
      "====> Epoch: 35 Average train loss: 0.0000\n",
      "====> Epoch: 35 Average val loss: 0.0000\n",
      "====> Epoch: 36 Average train loss: 0.0000\n",
      "====> Epoch: 36 Average val loss: 0.0000\n",
      "====> Epoch: 37 Average train loss: 0.0000\n",
      "====> Epoch: 37 Average val loss: 0.0000\n",
      "====> Epoch: 38 Average train loss: 0.0000\n",
      "====> Epoch: 38 Average val loss: 0.0000\n",
      "====> Epoch: 39 Average train loss: 0.0000\n",
      "====> Epoch: 39 Average val loss: 0.0000\n",
      "====> Epoch: 40 Average train loss: 0.0000\n",
      "====> Epoch: 40 Average val loss: 0.0000\n",
      "====> Epoch: 41 Average train loss: 0.0000\n",
      "====> Epoch: 41 Average val loss: 0.0000\n",
      "====> Epoch: 42 Average train loss: 0.0000\n",
      "====> Epoch: 42 Average val loss: 0.0000\n",
      "====> Epoch: 43 Average train loss: 0.0000\n",
      "====> Epoch: 43 Average val loss: 0.0000\n",
      "====> Epoch: 44 Average train loss: 0.0000\n",
      "====> Epoch: 44 Average val loss: 0.0000\n",
      "====> Epoch: 45 Average train loss: 0.0000\n",
      "====> Epoch: 45 Average val loss: 0.0000\n",
      "====> Epoch: 46 Average train loss: 0.0000\n",
      "====> Epoch: 46 Average val loss: 0.0000\n",
      "====> Epoch: 47 Average train loss: 0.0000\n",
      "====> Epoch: 47 Average val loss: 0.0000\n",
      "====> Epoch: 48 Average train loss: 0.0000\n",
      "====> Epoch: 48 Average val loss: 0.0000\n",
      "====> Epoch: 49 Average train loss: 0.0000\n",
      "====> Epoch: 49 Average val loss: 0.0000\n",
      "====> Epoch: 50 Average train loss: 0.0000\n",
      "====> Epoch: 50 Average val loss: 0.0000\n",
      "reconstruction error=1.5997795990548644e-08\n",
      "iteration 1, reconstruction error: 1.5997795990548644e-08, decrease = 0.0, unnormalized = 1.4901161193847656e-08\n",
      "PARAFAC converged after 1 iterations\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/ma/core.py:2832: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  order=order, subok=True, ndmin=ndmin)\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhYAL8vxkiAh"
   },
   "source": [
    "# Generate Some Plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jNeUaqjwhcj7",
    "outputId": "c9bde5ba-8a97-4f73-d3c2-ae15b3af5053",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "\"\"\"\n",
    "### Make results plot to compare methods\n",
    "\"\"\"\n",
    "# %%\n",
    "\n",
    "all_results = np.stack(\n",
    "    [linear_cca_results, gcca_results, mcca_results, pls_results, pmd_results, elastic_results,\n",
    "     scca_results, kernel_reg_results, kernel_poly_results,\n",
    "     kernel_gaussian_results, dcca_results, dgcca_results, dmcca_results, dvcca_model_results,\n",
    "     dcca_conv_results, dtcca_results],\n",
    "    axis=0)\n",
    "all_labels = ['linear', 'gcca', 'mcca', 'pls', 'pmd', 'elastic', 'scca', 'linear kernel', 'polynomial kernel',\n",
    "              'gaussian kernel', 'deep CCA', 'deep generalized CCA', 'deep multiset CCA', 'deep VCCA',\n",
    "              'deep convolutional cca', 'DTCCA']\n",
    "\n",
    "from cca_zoo import plot_utils\n",
    "\n",
    "plot_utils.plot_results(all_results, all_labels)\n",
    "plt.show()"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py:789: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  points = np.array(args, dtype=float).reshape(2, 2)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py:789: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  points = np.array(args, dtype=float).reshape(2, 2)\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1wgDHziqYJZX"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}